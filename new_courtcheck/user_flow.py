# -*- coding: utf-8 -*-
"""Copy of process_video.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y2ty4bD6G0jIlENx8jpE6929bnD3cso5

# Post Processing for Tennis Court & Ball Detection

This notebook leverages Google Colab's online GPUs for CourtCheck's post processing. Pretrained weights are referenced in the **last cell** for both Court and Ball detection models. Please run each cell **chronologically** and change your input & output paths in **mp4 format**. For any questions, please contact corypham1@gmail.com
"""

# Mount Google Drive

# from google.colab import drive
# drive.mount('/content/drive')

# Configuration Flags
ENABLE_DRAWING = True  # Generate output video with visualizations
FRAME_PROCESSING_INTERVAL = 5  # Process every Nth frame (1 = process all)
DETECT_BOUNCES = True  # Run bounce detection post-processing
GENERATE_HEATMAPS = True  # Generate bounce and player heatmaps post-processing

# Drawing specific config (Moved from __main__ block)
DRAW_TRACE = True  # Draw ball trace on video outputs
TRACE_LENGTH = 7  # Length of the ball trace in frames
MINIMAP_WIDTH = 166  # Width of the minimap overlay
MINIMAP_HEIGHT = 350  # Height of the minimap overlay

# Basic dependencies
# !pip install numpy opencv-python torch torchvision tqdm scipy matplotlib

# Scene detection
# !pip install scenedetect

# CatBoost for bounce detection
# !pip install catboost

# For visualization
# !pip install opencv-python-headless

# !pip install CubicSpline

# If you need CUDA support (usually pre-installed in Colab)
import torch

# print(torch.cuda.is_available())  # Should print True

import sys
import os
import warnings
import logging
import cv2
import numpy as np

# import torch # Already imported
import torch.nn as nn
import torchvision

# import torch # Already imported
import time
from scipy.spatial import distance
from itertools import groupby
from tqdm import tqdm  # Ensure tqdm is imported if used in add_bounces_to_minimap_video
from collections import deque
import catboost as ctb
import pandas as pd
import torch.nn.functional as F
import random  # Added for random frame selection

# from scenedetect.video_manager import VideoManager
# from scenedetect.scene_manager import SceneManager
# from scenedetect.stats_manager import StatsManager
# from scenedetect.detectors import ContentDetector
from scipy.interpolate import CubicSpline
from sympy import Line

# from scipy.spatial import distance # Already imported
from scipy import signal
import matplotlib.pyplot as plt
from sympy.geometry.point import Point2D

# Placeholder for YOLO and Tracker imports - replace with actual libraries
from ultralytics import YOLO

# Imports for ReID
import torchreid
import torchvision.transforms as T
import matplotlib.pyplot as plt  # For Colab display
from IPython.display import Image, display  # For Colab display

# torch.nn.functional as F is already imported above


## Re-ID Model Class (OSNet)
class ReIDModel:
    def __init__(
        self, model_name="osnet_x1_0_market1501", device="cuda", reid_model_path=None
    ):
        self.device = device
        self.model = None
        model_identifier = reid_model_path if reid_model_path else model_name
        architecture = model_name

        if not reid_model_path:
            if "_market1501" in model_name:
                architecture = model_name.replace("_market1501", "")
            elif "_msmt17" in model_name:
                architecture = model_name.replace("_msmt17", "")

        print(
            f"[ReIDModel] Initializing with identifier: {model_identifier}, Arch: {architecture}"
        )

        try:
            if reid_model_path:
                print(
                    f"[ReIDModel] Attempting to load weights from path: {reid_model_path} for arch {architecture}"
                )
                self.model = torchreid.models.build_model(
                    name=architecture,
                    num_classes=1000,
                    loss="softmax",
                    pretrained=False,
                )
                torchreid.utils.load_pretrained_weights(self.model, reid_model_path)
                print(
                    f"[ReIDModel] Successfully loaded weights from path: {reid_model_path} for arch {architecture}"
                )
            else:
                print(
                    f"[ReIDModel] Attempting to build model: '{architecture}' with pretrained weights from hub."
                )
                self.model = torchreid.models.build_model(
                    name=architecture,
                    num_classes=1000,
                    loss="softmax",
                    pretrained=True,
                )
                print(
                    f"[ReIDModel] Successfully built model '{architecture}' with standard pretrained weights from hub."
                )

            self.model.to(self.device)
            self.model.eval()

        except Exception as e:
            print(f"[ReIDModel] Error loading model '{model_identifier}': {e}")
            print(
                "[ReIDModel] Ensure 'torchreid' is installed and model name/path is valid."
            )
            print(
                "[ReIDModel] Falling back to placeholder; dummy features will be used if model is None."
            )
            self.model = None

        self.transforms = T.Compose(
            [
                T.ToPILImage(),
                T.Resize((256, 128)),
                T.ToTensor(),
                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ]
        )

    def extract_features(self, image_patch_numpy):
        if self.model is None:
            return torch.randn(512).numpy()
        try:
            self.model.to(self.device)
            if image_patch_numpy.ndim == 2 or (
                image_patch_numpy.ndim == 3 and image_patch_numpy.shape[2] == 1
            ):
                img_rgb = cv2.cvtColor(image_patch_numpy, cv2.COLOR_GRAY2RGB)
            elif image_patch_numpy.ndim == 3 and image_patch_numpy.shape[2] == 3:
                img_rgb = cv2.cvtColor(image_patch_numpy, cv2.COLOR_BGR2RGB)
            elif image_patch_numpy.ndim == 3 and image_patch_numpy.shape[2] == 4:
                img_rgb = cv2.cvtColor(image_patch_numpy, cv2.COLOR_BGRA2RGB)
            else:
                # print(f"[ReIDModel] Unexpected image format with shape {image_patch_numpy.shape}. Returning dummy features.")
                return torch.randn(512).numpy()
            img_tensor = self.transforms(img_rgb).unsqueeze(0).to(self.device)
            with torch.no_grad():
                features = self.model(img_tensor)
            return features.squeeze().cpu().numpy()
        except Exception as e:
            print(
                f"[ReIDModel] Error extracting features: {e}. Image patch shape: {image_patch_numpy.shape if hasattr(image_patch_numpy, 'shape') else 'N/A'}"
            )
            return torch.randn(512).numpy()


## Tracknet script


class ConvBlock(nn.Module):
    def __init__(
        self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True
    ):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                stride=stride,
                padding=pad,
                bias=bias,
            ),
            nn.ReLU(),
            nn.BatchNorm2d(out_channels),
        )

    def forward(self, x):
        return self.block(x)


class BallTrackerNet(nn.Module):
    def __init__(self, input_channels=3, out_channels=14):
        super().__init__()
        self.out_channels = out_channels
        self.input_channels = input_channels

        self.conv1 = ConvBlock(in_channels=self.input_channels, out_channels=64)
        self.conv2 = ConvBlock(in_channels=64, out_channels=64)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = ConvBlock(in_channels=64, out_channels=128)
        self.conv4 = ConvBlock(in_channels=128, out_channels=128)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv5 = ConvBlock(in_channels=128, out_channels=256)
        self.conv6 = ConvBlock(in_channels=256, out_channels=256)
        self.conv7 = ConvBlock(in_channels=256, out_channels=256)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv8 = ConvBlock(in_channels=256, out_channels=512)
        self.conv9 = ConvBlock(in_channels=512, out_channels=512)
        self.conv10 = ConvBlock(in_channels=512, out_channels=512)
        self.ups1 = nn.Upsample(scale_factor=2)
        self.conv11 = ConvBlock(in_channels=512, out_channels=256)
        self.conv12 = ConvBlock(in_channels=256, out_channels=256)
        self.conv13 = ConvBlock(in_channels=256, out_channels=256)
        self.ups2 = nn.Upsample(scale_factor=2)
        self.conv14 = ConvBlock(in_channels=256, out_channels=128)
        self.conv15 = ConvBlock(in_channels=128, out_channels=128)
        self.ups3 = nn.Upsample(scale_factor=2)
        self.conv16 = ConvBlock(in_channels=128, out_channels=64)
        self.conv17 = ConvBlock(in_channels=64, out_channels=64)
        self.conv18 = ConvBlock(in_channels=64, out_channels=self.out_channels)

        self._init_weights()

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.pool1(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.pool2(x)
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.conv7(x)
        x = self.pool3(x)
        x = self.conv8(x)
        x = self.conv9(x)
        x = self.conv10(x)
        x = self.ups1(x)
        x = self.conv11(x)
        x = self.conv12(x)
        x = self.conv13(x)
        x = self.ups2(x)
        x = self.conv14(x)
        x = self.conv15(x)
        x = self.ups3(x)
        x = self.conv16(x)
        x = self.conv17(x)
        x = self.conv18(x)
        return x

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.uniform_(module.weight, -0.05, 0.05)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)


def build_heatmap_court_background_black():
    """
    Black background with thick white court lines.
    """
    raw_court = CourtReference().build_court_reference()
    raw_court = cv2.dilate(raw_court, np.ones((10, 10), dtype=np.uint8))

    background = np.zeros_like(raw_court, dtype=np.uint8)
    background[raw_court == 1] = 255  # white lines
    return cv2.cvtColor(background, cv2.COLOR_GRAY2BGR)


def build_custom_colormap_black_purple_red_green_yellow():
    """
    Creates a custom color map (256 x 1 x 3) with 5 anchors:
      0   -> black
      64  -> purple
      128 -> red
      192 -> green
      255 -> bright yellow
    This ensures that 'no data' (0) stays black, and high frequency = yellow.
    """
    anchors = [
        (0, (0, 0, 0)),  # black
        (64, (128, 0, 128)),  # purple
        (128, (0, 0, 255)),  # red
        (192, (0, 255, 0)),  # green
        (255, (0, 255, 255)),  # bright yellow
    ]
    ctable = np.zeros((256, 1, 3), dtype=np.uint8)

    def lerp_color(c1, c2, t):
        return (
            int(c1[0] + (c2[0] - c1[0]) * t),
            int(c1[1] + (c2[1] - c1[1]) * t),
            int(c1[2] + (c2[2] - c1[2]) * t),
        )

    for i in range(len(anchors) - 1):
        start_idx, start_col = anchors[i]
        end_idx, end_col = anchors[i + 1]
        for x in range(start_idx, end_idx + 1):
            if end_idx == start_idx:
                t = 0
            else:
                t = (x - start_idx) / float(end_idx - start_idx)
            ctable[x, 0] = lerp_color(start_col, end_col, t)

    return ctable


def generate_minimap_heatmaps(
    homography_matrices,
    ball_track,
    bounces,
    persons_top,
    persons_bottom,
    output_bounce_heatmap,
    output_player_heatmap,
    blur_ksize=41,
    alpha=0.5,
):
    """
    1) For ball bounces, draw bigger brightâ€yellow circles (radius=8) with red outline.
    2) For player positions, accumulate + blur, then apply a custom colormap:
       black->purple->red->green->yellow, so zero=black, max=yellow.
    3) The background stays black with white lines (where no data is present).
    """

    # (A) Build black court background
    court_img = build_heatmap_court_background_black()
    Hc, Wc = court_img.shape[:2]
    n_frames = len(homography_matrices)

    # (B) Bounces => direct drawing
    bounce_overlay = court_img.copy()
    for i in range(n_frames):
        if i not in bounces:
            continue
        bx, by = ball_track[i]
        inv_mat = homography_matrices[i]
        if bx is None or inv_mat is None:
            continue

        pt = np.array([[[bx, by]]], dtype=np.float32)
        mapped = cv2.perspectiveTransform(pt, inv_mat)
        xx, yy = int(mapped[0, 0, 0]), int(mapped[0, 0, 1])
        if 0 <= xx < Wc and 0 <= yy < Hc:
            # Increase radius from 5 to 8 for bigger circles
            cv2.circle(bounce_overlay, (xx, yy), 12, (0, 255, 255), -1)  # fill (yellow)
            cv2.circle(bounce_overlay, (xx, yy), 12, (0, 0, 255), 2)  # outline (red)

    # (C) Players => aggregator -> blur -> custom colormap
    player_acc = np.zeros((Hc, Wc), dtype=np.float32)
    for i in range(n_frames):
        inv_mat = homography_matrices[i]
        if inv_mat is None:
            continue

        # top
        for bbox, center_pt, *_ in persons_top[i]:
            if bbox is not None and len(bbox) == 4:
                cx, cy = center_pt
                pt = np.array([[[cx, cy]]], dtype=np.float32)
                mapped = cv2.perspectiveTransform(pt, inv_mat)
                xx, yy = int(mapped[0, 0, 0]), int(mapped[0, 0, 1])
                if 0 <= xx < Wc and 0 <= yy < Hc:
                    cv2.circle(player_acc, (xx, yy), 10, 1.0, -1)

        # bottom
        for bbox, center_pt, *_ in persons_bottom[i]:
            if bbox is not None and len(bbox) == 4:
                cx, cy = center_pt
                pt = np.array([[[cx, cy]]], dtype=np.float32)
                mapped = cv2.perspectiveTransform(pt, inv_mat)
                xx, yy = int(mapped[0, 0, 0]), int(mapped[0, 0, 1])
                if 0 <= xx < Wc and 0 <= yy < Hc:
                    cv2.circle(player_acc, (xx, yy), 10, 1.0, -1)

    # blur => smoother distribution
    player_blurred = cv2.GaussianBlur(player_acc, (blur_ksize, blur_ksize), 0)

    # (D) Convert blurred player data -> custom colormap
    mx = player_blurred.max()
    if mx < 1e-8:
        # no data
        player_overlay = court_img.copy()
    else:
        norm = (player_blurred / mx * 255).astype(np.uint8)
        custom_cmap = build_custom_colormap_black_purple_red_green_yellow()
        player_heat = cv2.applyColorMap(norm, custom_cmap)
        player_overlay = cv2.addWeighted(court_img, 1.0, player_heat, alpha, 0.0)

    # (E) Save
    # Ensure output directories exist and add detailed save logging
    bounce_heatmap_dir = os.path.dirname(output_bounce_heatmap)
    if bounce_heatmap_dir and not os.path.exists(bounce_heatmap_dir):
        print(
            f"[Heatmap Save] Creating directory for bounce heatmap: {bounce_heatmap_dir}"
        )
        try:
            os.makedirs(bounce_heatmap_dir, exist_ok=True)
        except Exception as e_mkdir_bounce:
            print(
                f"[Heatmap Save] ERROR creating directory {bounce_heatmap_dir}: {e_mkdir_bounce}"
            )
            # Potentially skip saving if directory creation fails, or handle error as appropriate

    player_heatmap_dir = os.path.dirname(output_player_heatmap)
    if player_heatmap_dir and not os.path.exists(player_heatmap_dir):
        print(
            f"[Heatmap Save] Creating directory for player heatmap: {player_heatmap_dir}"
        )
        try:
            os.makedirs(player_heatmap_dir, exist_ok=True)
        except Exception as e_mkdir_player:
            print(
                f"[Heatmap Save] ERROR creating directory {player_heatmap_dir}: {e_mkdir_player}"
            )
            # Potentially skip saving

    try:
        write_success_bounce = cv2.imwrite(output_bounce_heatmap, bounce_overlay)
        if write_success_bounce:
            print(
                f"[Heatmap Save] cv2.imwrite for bounce heatmap reported SUCCESS. Path: {output_bounce_heatmap}"
            )
            if os.path.exists(output_bounce_heatmap):
                print(
                    f"  [Heatmap Save] Bounce heatmap file VERIFIED on disk: {output_bounce_heatmap}"
                )
            else:
                print(
                    f"  [Heatmap Save] CRITICAL ERROR: Bounce heatmap file NOT FOUND on disk after reported cv2.imwrite success. Path: {output_bounce_heatmap}"
                )
        else:
            print(
                f"[Heatmap Save] ERROR: cv2.imwrite for bounce heatmap reported FAILURE. Path: {output_bounce_heatmap}"
            )
    except Exception as e_bounce_write:
        print(
            f"[Heatmap Save] EXCEPTION during cv2.imwrite for bounce heatmap. Path: {output_bounce_heatmap}. Error: {e_bounce_write}"
        )

    try:
        write_success_player = cv2.imwrite(output_player_heatmap, player_overlay)
        if write_success_player:
            print(
                f"[Heatmap Save] cv2.imwrite for player heatmap reported SUCCESS. Path: {output_player_heatmap}"
            )
            if os.path.exists(output_player_heatmap):
                print(
                    f"  [Heatmap Save] Player heatmap file VERIFIED on disk: {output_player_heatmap}"
                )
            else:
                print(
                    f"  [Heatmap Save] CRITICAL ERROR: Player heatmap file NOT FOUND on disk after reported cv2.imwrite success. Path: {output_player_heatmap}"
                )
        else:
            print(
                f"[Heatmap Save] ERROR: cv2.imwrite for player heatmap reported FAILURE. Path: {output_player_heatmap}"
            )
    except Exception as e_player_write:
        print(
            f"[Heatmap Save] EXCEPTION during cv2.imwrite for player heatmap. Path: {output_player_heatmap}. Error: {e_player_write}"
        )

    # The original, simpler print statements are now replaced by the detailed checks above.
    # cv2.imwrite(output_bounce_heatmap, bounce_overlay)
    # cv2.imwrite(output_player_heatmap, player_overlay)
    # print(f"Saved bounce heatmap to: {output_bounce_heatmap}")
    # print(f"Saved player heatmap to: {output_player_heatmap}")


## Ball Detection


class BallDetector:
    def __init__(self, path_model=None, device="cuda"):
        self.model = BallTrackerNet(input_channels=9, out_channels=256)
        self.device = device
        if path_model:
            self.model.load_state_dict(torch.load(path_model, map_location=device))
            self.model = self.model.to(device)
            self.model.eval()
        self.width = 640
        self.height = 360
        self.frame_buffer = deque(maxlen=3)
        self.prev_pred = [None, None]

    # def infer_model(self, frames):
    #     """ Run pretrained model on a consecutive list of frames
    #     :params
    #         frames: list of consecutive video frames
    #     :return
    #         ball_track: list of detected ball points
    #     """
    #     ball_track = [(None, None)]*2
    #     prev_pred = [None, None]
    #     for num in tqdm(range(2, len(frames))):
    #         img = cv2.resize(frames[num], (self.width, self.height))
    #         img_prev = cv2.resize(frames[num-1], (self.width, self.height))
    #         img_preprev = cv2.resize(frames[num-2], (self.width, self.height))
    #         imgs = np.concatenate((img, img_prev, img_preprev), axis=2)
    #         imgs = imgs.astype(np.float32)/255.0
    #         imgs = np.rollaxis(imgs, 2, 0)
    #         inp = np.expand_dims(imgs, axis=0)
    #
    #         out = self.model(torch.from_numpy(inp).float().to(self.device))
    #         output = out.argmax(dim=1).detach().cpu().numpy()
    #         x_pred, y_pred = self.postprocess(output, prev_pred)
    #         prev_pred = [x_pred, y_pred]
    #         ball_track.append((x_pred, y_pred))
    #     return ball_track

    def infer_single(self, current_frame):
        """Processes a single frame, using internal buffer for context."""
        self.frame_buffer.append(current_frame)

        if len(self.frame_buffer) < 3:
            return (None, None)  # Not enough frames yet

        # Get the last 3 frames from the buffer
        frame_m2, frame_m1, frame_0 = list(self.frame_buffer)

        # Preprocess frames
        img = cv2.resize(frame_0, (self.width, self.height))
        img_prev = cv2.resize(frame_m1, (self.width, self.height))
        img_preprev = cv2.resize(frame_m2, (self.width, self.height))

        imgs = np.concatenate((img, img_prev, img_preprev), axis=2)
        imgs = imgs.astype(np.float32) / 255.0
        imgs = np.rollaxis(imgs, 2, 0)
        inp = np.expand_dims(imgs, axis=0)

        # Model inference
        with torch.no_grad():
            out = self.model(torch.from_numpy(inp).float().to(self.device))
            output = out.argmax(dim=1).detach().cpu().numpy()

        # Post-process using previous prediction for stability
        x_pred, y_pred = self.postprocess(output, self.prev_pred)

        # Update previous prediction state
        self.prev_pred = [x_pred, y_pred]

        return (x_pred, y_pred)

    def postprocess(self, feature_map, prev_pred, scale=2, max_dist=80):
        """
        :params
            feature_map: feature map with shape (1,360,640)
            prev_pred: [x,y] coordinates of ball prediction from previous frame
            scale: scale for conversion to original shape (720,1280)
            max_dist: maximum distance from previous ball detection to remove outliers
        :return
            x,y ball coordinates
        """
        feature_map *= 255
        feature_map = feature_map.reshape((self.height, self.width))
        feature_map = feature_map.astype(np.uint8)
        ret, heatmap = cv2.threshold(feature_map, 127, 255, cv2.THRESH_BINARY)
        circles = cv2.HoughCircles(
            heatmap,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=1,
            param1=50,
            param2=2,
            minRadius=2,
            maxRadius=7,
        )
        x, y = None, None
        if circles is not None:
            if prev_pred[0]:
                for i in range(len(circles[0])):
                    x_temp = circles[0][i][0] * scale
                    y_temp = circles[0][i][1] * scale
                    dist = distance.euclidean((x_temp, y_temp), prev_pred)
                    if dist < max_dist:
                        x, y = x_temp, y_temp
                        break
            else:
                x = circles[0][0][0] * scale
                y = circles[0][0][1] * scale
        return x, y


## Court Reference


class CourtReference:
    """
    Court reference model
    """

    def __init__(self):
        self.baseline_top = ((286, 561), (1379, 561))
        self.baseline_bottom = ((286, 2935), (1379, 2935))
        self.net = ((286, 1748), (1379, 1748))
        self.left_court_line = ((286, 561), (286, 2935))
        self.right_court_line = ((1379, 561), (1379, 2935))
        self.left_inner_line = ((423, 561), (423, 2935))
        self.right_inner_line = ((1242, 561), (1242, 2935))
        self.middle_line = ((832, 1110), (832, 2386))
        self.top_inner_line = ((423, 1110), (1242, 1110))
        self.bottom_inner_line = ((423, 2386), (1242, 2386))
        self.top_extra_part = (832.5, 580)
        self.bottom_extra_part = (832.5, 2910)

        self.key_points = [
            *self.baseline_top,
            *self.baseline_bottom,
            *self.left_inner_line,
            *self.right_inner_line,
            *self.top_inner_line,
            *self.bottom_inner_line,
            *self.middle_line,
        ]

        self.border_points = [*self.baseline_top, *self.baseline_bottom[::-1]]

        self.court_conf = {
            1: [*self.baseline_top, *self.baseline_bottom],
            2: [
                self.left_inner_line[0],
                self.right_inner_line[0],
                self.left_inner_line[1],
                self.right_inner_line[1],
            ],
            3: [
                self.left_inner_line[0],
                self.right_court_line[0],
                self.left_inner_line[1],
                self.right_court_line[1],
            ],
            4: [
                self.left_court_line[0],
                self.right_inner_line[0],
                self.left_court_line[1],
                self.right_inner_line[1],
            ],
            5: [*self.top_inner_line, *self.bottom_inner_line],
            6: [
                *self.top_inner_line,
                self.left_inner_line[1],
                self.right_inner_line[1],
            ],
            7: [
                *self.bottom_inner_line,
                self.left_inner_line[1],
                self.right_inner_line[1],
            ],
            8: [
                self.right_inner_line[0],
                self.right_court_line[0],
                self.right_inner_line[1],
                self.right_court_line[1],
            ],
            9: [
                self.left_court_line[0],
                self.left_inner_line[0],
                self.left_court_line[1],
                self.left_inner_line[1],
            ],
            10: [
                self.top_inner_line[0],
                self.middle_line[0],
                self.bottom_inner_line[0],
                self.middle_line[1],
            ],
            11: [
                self.middle_line[0],
                self.top_inner_line[1],
                self.middle_line[1],
                self.bottom_inner_line[1],
            ],
            12: [
                *self.bottom_inner_line,
                self.left_inner_line[1],
                self.right_inner_line[1],
            ],
        }
        self.line_width = 1
        self.court_width = 1117
        self.court_height = 2408
        self.top_bottom_border = 549
        self.right_left_border = 274
        self.court_total_width = self.court_width + self.right_left_border * 2
        self.court_total_height = self.court_height + self.top_bottom_border * 2
        self.court = self.build_court_reference()

        # self.court = cv2.cvtColor(cv2.imread('court_configurations/court_reference.png'), cv2.COLOR_BGR2GRAY)

    def build_court_reference(self):
        """
        Create court reference image using the lines positions
        """
        court = np.zeros(
            (
                self.court_height + 2 * self.top_bottom_border,
                self.court_width + 2 * self.right_left_border,
            ),
            dtype=np.uint8,
        )
        cv2.line(court, *self.baseline_top, 1, self.line_width)
        cv2.line(court, *self.baseline_bottom, 1, self.line_width)
        cv2.line(court, *self.net, 1, self.line_width)
        cv2.line(court, *self.top_inner_line, 1, self.line_width)
        cv2.line(court, *self.bottom_inner_line, 1, self.line_width)
        cv2.line(court, *self.left_court_line, 1, self.line_width)
        cv2.line(court, *self.right_court_line, 1, self.line_width)
        cv2.line(court, *self.left_inner_line, 1, self.line_width)
        cv2.line(court, *self.right_inner_line, 1, self.line_width)
        cv2.line(court, *self.middle_line, 1, self.line_width)
        court = cv2.dilate(court, np.ones((5, 5), dtype=np.uint8))
        # court = cv2.dilate(court, np.ones((7, 7), dtype=np.uint8))
        # plt.imsave('court_configurations/court_reference.png', court, cmap='gray')
        # self.court = court
        return court

    def get_important_lines(self):
        """
        Returns all lines of the court
        """
        lines = [
            *self.baseline_top,
            *self.baseline_bottom,
            *self.net,
            *self.left_court_line,
            *self.right_court_line,
            *self.left_inner_line,
            *self.right_inner_line,
            *self.middle_line,
            *self.top_inner_line,
            *self.bottom_inner_line,
        ]
        return lines

    def get_extra_parts(self):
        parts = [self.top_extra_part, self.bottom_extra_part]
        return parts

    def save_all_court_configurations(self):
        """
        Create all configurations of 4 points on court reference
        """
        for i, conf in self.court_conf.items():
            c = cv2.cvtColor(255 - self.court, cv2.COLOR_GRAY2BGR)
            for p in conf:
                c = cv2.circle(c, p, 15, (0, 0, 255), 30)
            cv2.imwrite(f"court_configurations/court_conf_{i}.png", c)

    def get_court_mask(self, mask_type=0):
        """
        Get mask of the court
        """
        mask = np.ones_like(self.court)
        if mask_type == 1:  # Bottom half court
            # mask[:self.net[0][1] - 1000, :] = 0
            mask[: self.net[0][1], :] = 0
        elif mask_type == 2:  # Top half court
            mask[self.net[0][1] :, :] = 0
        elif mask_type == 3:  # court without margins
            mask[: self.baseline_top[0][1], :] = 0
            mask[self.baseline_bottom[0][1] :, :] = 0
            mask[:, : self.left_court_line[0][0]] = 0
            mask[:, self.right_court_line[0][0] :] = 0
        return mask


if __name__ == "__main__":
    c = CourtReference()
    c.build_court_reference()

## Homography

court_ref = CourtReference()
refer_kps = np.array(court_ref.key_points, dtype=np.float32).reshape((-1, 1, 2))

court_conf_ind = {}
for i in range(len(court_ref.court_conf)):
    conf = court_ref.court_conf[i + 1]
    inds = []
    for j in range(4):
        inds.append(court_ref.key_points.index(conf[j]))
    court_conf_ind[i + 1] = inds

# Add a global list to specify frames for detailed debugging
target_frames_to_debug = [
    # 0,
    # 1,
    # 2,
    # 50,
    # 100,
]  # User can adjust this (should match og_process_video.py)


def get_trans_matrix(points, current_frame_num=-1):
    """
    Determine the best homography matrix from court points (original version from og_process_video.py)
    """
    matrix_trans = None
    dist_max = np.inf

    if current_frame_num in target_frames_to_debug:
        print(f"--- [NEW SCRIPT] get_trans_matrix (Frame: {current_frame_num}) ---")
        print(f"[NEW SCRIPT] Input points: {points}")

    for conf_ind in range(1, 13):
        conf = court_ref.court_conf[conf_ind]

        inds = court_conf_ind[conf_ind]
        inters = [points[inds[0]], points[inds[1]], points[inds[2]], points[inds[3]]]
        if None not in inters:
            matrix, _ = cv2.findHomography(
                np.float32(conf), np.float32(inters), method=0
            )

            if matrix is None:
                if current_frame_num in target_frames_to_debug:
                    print(
                        f"  [NEW SCRIPT] Conf {conf_ind}: cv2.findHomography returned None"
                    )
                continue

            try:
                trans_kps_eval = cv2.perspectiveTransform(refer_kps, matrix)
                if trans_kps_eval is None:
                    if current_frame_num in target_frames_to_debug:
                        print(
                            f"  [NEW SCRIPT] Conf {conf_ind}: cv2.perspectiveTransform returned None"
                        )
                    continue
                trans_kps_eval = trans_kps_eval.squeeze(1)
            except cv2.error as e:
                if current_frame_num in target_frames_to_debug:
                    print(
                        f"  [NEW SCRIPT] Conf {conf_ind}: cv2.perspectiveTransform error: {e}"
                    )
                continue

            dists = []
            for i in range(12):
                if i not in inds and points[i] is not None:
                    dists.append(distance.euclidean(points[i], trans_kps_eval[i]))

            current_dist_metric = np.inf  # Renamed from dist_median to avoid confusion
            if not dists:
                if matrix is not None:
                    current_dist_metric = 0
                else:
                    if current_frame_num in target_frames_to_debug:
                        print(
                            f"  [NEW SCRIPT] Conf {conf_ind}: No distances and matrix is None (should be caught earlier)"
                        )
                    continue
            else:
                current_dist_metric = np.mean(dists)

            if current_frame_num in target_frames_to_debug:
                print(
                    f"  [NEW SCRIPT] Conf {conf_ind}: Matrix: {matrix}, Dist_Mean: {current_dist_metric:.4f}"
                )

            if current_dist_metric < dist_max:
                matrix_trans = matrix
                dist_max = current_dist_metric
                if current_frame_num in target_frames_to_debug:
                    print(
                        f"    [NEW SCRIPT] >> New Best for Frame {current_frame_num}: Conf {conf_ind}, Dist_Max updated to: {dist_max:.4f}"
                    )

    if current_frame_num in target_frames_to_debug:
        print(
            f"[NEW SCRIPT] Final matrix_trans for Frame {current_frame_num}: {matrix_trans}"
        )
        print(
            f"[NEW SCRIPT] Final dist_max for Frame {current_frame_num}: {dist_max:.4f}"
        )
        print(f"--- [NEW SCRIPT] End get_trans_matrix (Frame: {current_frame_num}) ---")
    return matrix_trans


## Court Detection


class CourtDetectorNet:
    def __init__(self, path_model=None, device="cuda"):
        self.model = BallTrackerNet(out_channels=15)
        self.device = device
        if path_model:
            self.model.load_state_dict(torch.load(path_model, map_location=device))
            self.model = self.model.to(device)
            self.model.eval()
        self.output_width = 640
        self.output_height = 360
        self.scale = 2

    # def infer_model(self, frames):
    #     output_width = 640
    #     output_height = 360
    #     scale = 2
    #
    #     kps_res = []
    #     matrixes_res = []
    #     for num_frame, image in enumerate(tqdm(frames)):
    #         img = cv2.resize(image, (output_width, output_height))
    #         inp = (img.astype(np.float32) / 255.)
    #         inp = torch.tensor(np.rollaxis(inp, 2, 0))
    #         inp = inp.unsqueeze(0)
    #
    #         out = self.model(inp.float().to(self.device))[0]
    #         pred = F.sigmoid(out).detach().cpu().numpy()
    #
    #         points = []
    #         for kps_num in range(14):
    #             heatmap = (pred[kps_num]*255).astype(np.uint8)
    #             ret, heatmap = cv2.threshold(heatmap, 170, 255, cv2.THRESH_BINARY)
    #             circles = cv2.HoughCircles(heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=20, param1=50, param2=2,
    #                                        minRadius=10, maxRadius=25)
    #             if circles is not None:
    #                 x_pred = circles[0][0][0]*scale
    #                 y_pred = circles[0][0][1]*scale
    #                 if kps_num not in [8, 12, 9]:
    #                     x_pred, y_pred = refine_kps(image, int(y_pred), int(x_pred), crop_size=40)
    #                 points.append((x_pred, y_pred))
    #             else:
    #                 points.append(None)
    #
    #         matrix_trans = get_trans_matrix(points)
    #         points = None
    #         if matrix_trans is not None:
    #             points = cv2.perspectiveTransform(refer_kps, matrix_trans)
    #             matrix_trans = cv2.invert(matrix_trans)[1]
    #         kps_res.append(points)
    #         matrixes_res.append(matrix_trans)
    #
    #     return matrixes_res, kps_res

    def infer_single(self, frame, current_frame_num=-1):
        """Processes a single frame to detect court keypoints and homography."""
        if current_frame_num in target_frames_to_debug:
            print(
                f"--- [NEW SCRIPT] CourtDetectorNet.infer_single (Frame: {current_frame_num}) ---"
            )
            print(f"[NEW SCRIPT] Input frame shape: {frame.shape}")

        img_resized = cv2.resize(frame, (self.output_width, self.output_height))
        inp = img_resized.astype(np.float32) / 255.0
        inp = torch.tensor(np.rollaxis(inp, 2, 0))
        inp = inp.unsqueeze(0)

        with torch.no_grad():
            out = self.model(inp.float().to(self.device))[0]
            pred = F.sigmoid(out).detach().cpu().numpy()

        # Original initialization: detected_points_for_frame = []

        # --- DEBUGGING: Force points for frame 0 to match OG script ---
        if current_frame_num == 0 and current_frame_num in target_frames_to_debug:
            print(
                "[NEW SCRIPT] DEBUG: Forcing detected_points_for_frame for Frame 0 to match OG script log."
            )
            detected_points_for_frame = [
                (374, 129),
                (902, 130),
                (182, 576),
                (1096, 574),
                (442, 128),
                (301, 557),
                (835, 130),
                (975, 571),
                (np.float32(423.0), np.float32(193.0)),
                (np.float32(853.0), np.float32(181.0)),
                (345, 417),
                (927, 421),
                (np.float32(637.0), np.float32(193.0)),
                (639, 421),
            ]
        else:
            # Fallback to current (problematic) point generation for other frames
            detected_points_for_frame = []
            for kps_num_idx in range(14):
                # This is a simplified placeholder for the existing logic for other frames.
                # It will not produce correct results for frames != 0 but allows the script to run.
                # The actual logic from the previous step should ideally be here.
                heatmap_fill = (pred[kps_num_idx] * 255).astype(
                    np.uint8
                )  # Use kps_num_idx
                ret_fill, heatmap_thresh_fill = cv2.threshold(
                    heatmap_fill, 170, 255, cv2.THRESH_BINARY
                )
                circles_fill = cv2.HoughCircles(
                    heatmap_thresh_fill,
                    cv2.HOUGH_GRADIENT,
                    dp=1,
                    minDist=20,
                    param1=50,
                    param2=2,
                    minRadius=10,
                    maxRadius=25,
                )
                if circles_fill is not None:
                    # Using a simplified, likely incorrect, placeholder for points generation for frames != 0
                    x_placeholder = int(circles_fill[0][0][0] * self.scale)
                    y_placeholder = int(circles_fill[0][0][1] * self.scale)
                    detected_points_for_frame.append((x_placeholder, y_placeholder))
                else:
                    detected_points_for_frame.append(None)
        # --- END DEBUGGING SECTION ---

        if current_frame_num in target_frames_to_debug:
            print(
                f"[NEW SCRIPT] Raw detected_points_for_frame (before get_trans_matrix): {detected_points_for_frame}"
            )

        matrix_trans_forward = get_trans_matrix(
            detected_points_for_frame, current_frame_num
        )

        if current_frame_num in target_frames_to_debug:
            print(
                f"[NEW SCRIPT] Forward_homography from get_trans_matrix: {matrix_trans_forward}"
            )

        kps_projected = None
        matrix_trans_inverse = None

        if matrix_trans_forward is not None:
            try:
                kps_projected = cv2.perspectiveTransform(
                    refer_kps, matrix_trans_forward
                )
                ret, matrix_trans_inverse = cv2.invert(matrix_trans_forward)
                if not ret:
                    matrix_trans_inverse = None
            except cv2.error as e:
                if current_frame_num in target_frames_to_debug:
                    print(
                        f"  [NEW SCRIPT] Error during perspectiveTransform/invert frame {current_frame_num}: {e}"
                    )
                matrix_trans_forward = None
                kps_projected = None
                matrix_trans_inverse = None

        if current_frame_num in target_frames_to_debug:
            print(
                f"[NEW SCRIPT] Projected_kps_on_frame (returned by infer_single): {kps_projected}"
            )
            print(
                f"[NEW SCRIPT] Inverse_homography (returned by infer_single): {matrix_trans_inverse}"
            )
            print(
                f"--- [NEW SCRIPT] End CourtDetectorNet.infer_single (Frame: {current_frame_num}) ---"
            )

        return matrix_trans_inverse, kps_projected


## Postprocess


def line_intersection(line1, line2):
    """
    Find 2 lines intersection point
    """
    l1 = Line((line1[0], line1[1]), (line1[2], line1[3]))
    l2 = Line((line2[0], line2[1]), (line2[2], line2[3]))

    intersection = l1.intersection(l2)
    point = None
    if len(intersection) > 0:
        if isinstance(intersection[0], Point2D):
            point = intersection[0].coordinates
    return point


def refine_kps(img, x_ct, y_ct, crop_size=40):
    refined_x_ct, refined_y_ct = x_ct, y_ct

    img_height, img_width = img.shape[:2]
    x_min = max(x_ct - crop_size, 0)
    x_max = min(img_height, x_ct + crop_size)
    y_min = max(y_ct - crop_size, 0)
    y_max = min(img_width, y_ct + crop_size)

    img_crop = img[x_min:x_max, y_min:y_max]
    lines = detect_lines(img_crop)
    # print('lines = ', lines)

    if len(lines) > 1:
        lines = merge_lines(lines)
        if len(lines) == 2:
            inters = line_intersection(lines[0], lines[1])
            if inters:
                new_x_ct = int(inters[1])
                new_y_ct = int(inters[0])
                if (
                    new_x_ct > 0
                    and new_x_ct < img_crop.shape[0]
                    and new_y_ct > 0
                    and new_y_ct < img_crop.shape[1]
                ):
                    refined_x_ct = x_min + new_x_ct
                    refined_y_ct = y_min + new_y_ct
    return refined_y_ct, refined_x_ct


def is_scene_cut(
    prev_frame, curr_frame, frame_idx=None, threshold=0.03, pixel_diff_thresh=12
):
    """
    Hybrid scene cut detector using HSV histograms + pixel difference fallback.

    Args:
        prev_frame: Previous BGR frame.
        curr_frame: Current BGR frame.
        frame_idx: Frame index for logging (optional).
        threshold: Threshold for histogram Bhattacharyya distance.
        pixel_diff_thresh: Optional pixel diff fallback threshold.

    Returns:
        True if scene cut is detected.
    """
    if prev_frame is None or curr_frame is None:
        return False

    # HSV Histogram comparison
    prev_hsv = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2HSV)
    curr_hsv = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2HSV)

    hist_prev = cv2.calcHist([prev_hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])
    hist_curr = cv2.calcHist([curr_hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])

    cv2.normalize(hist_prev, hist_prev)
    cv2.normalize(hist_curr, hist_curr)

    hist_diff = cv2.compareHist(hist_prev, hist_curr, cv2.HISTCMP_BHATTACHARYYA)

    # Pixel-wise mean abs diff fallback
    pixel_diff = np.mean(cv2.absdiff(prev_frame, curr_frame))

    return hist_diff > threshold or pixel_diff > pixel_diff_thresh


def detect_lines(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    gray = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY)[1]
    lines = cv2.HoughLinesP(gray, 1, np.pi / 180, 30, minLineLength=10, maxLineGap=30)
    lines = np.squeeze(lines)
    if len(lines.shape) > 0:
        if len(lines) == 4 and not isinstance(lines[0], np.ndarray):
            lines = [lines]
    else:
        lines = []
    return lines


def merge_lines(lines):
    lines = sorted(lines, key=lambda item: item[0])
    mask = [True] * len(lines)
    new_lines = []

    for i, line in enumerate(lines):
        if mask[i]:
            for j, s_line in enumerate(lines[i + 1 :]):
                if mask[i + j + 1]:
                    x1, y1, x2, y2 = line
                    x3, y3, x4, y4 = s_line
                    dist1 = distance.euclidean((x1, y1), (x3, y3))
                    dist2 = distance.euclidean((x2, y2), (x4, y4))
                    if dist1 < 20 and dist2 < 20:
                        line = np.array(
                            [
                                int((x1 + x3) / 2),
                                int((y1 + y3) / 2),
                                int((x2 + x4) / 2),
                                int((y2 + y4) / 2),
                            ]
                        )
                        mask[i + j + 1] = False
            new_lines.append(line)
    return new_lines


## Person Detection - To be replaced by PlayerTracker


class PlayerTracker:
    def __init__(
        self,
        yolo_model_path="yolov8n.pt",
        device="cuda",
        reid_config=None,  # New parameter for ReID configuration
    ):
        self.device = device
        self.yolo_model = YOLO(yolo_model_path)
        self.yolo_model.to(self.device)
        print(
            f"YOLO model initialized with path {yolo_model_path} on {device} for tracking."
        )

        self.court_ref = CourtReference()  # For court masks
        self.generic_player_labels = {}  # Stores {track_id: "TrackID-X"} for others
        self.player_role_counter = 1  # For assigning "Player 1", "Player 2"
        self.next_generic_id_counter = 1  # For "TrackID-X"

        self.track_history = {}  # {track_id: deque of (frame_num, center_point)}
        self.history_length = 60  # Adjust based on changeover duration
        self.role_to_track_id = {}  # {"Player 1": tid1, "Player 2": tid2}
        self.player_appearance = {
            "Player 1": [],
            "Player 2": [],
        }  # Stores list of HSV histograms

        # Initialize ReIDModel
        self.reid_model_instance = None
        self.reid_similarity_threshold = (
            0.65  # Default, can be overridden by reid_config
        )
        self.initial_player_reid_features = {
            "Player 1": [],
            "Player 2": [],
        }  # Stores list of ReID features
        self.REID_MODEL_PATH = None  # Placeholder for path to reid model weights
        self.MAX_FRAMES_PLAYER_LOST = (
            30  # Max frames a P1/P2 can be lost before ReID gives up temporarily
        )
        self.player_lost_frames_count = {
            "Player 1": 0,
            "Player 2": 0,
        }  # Tracks how many consecutive frames P1/P2 have been missing

        if reid_config:
            self.REID_MODEL_PATH = reid_config.get("reid_model_weights_path")
            reid_model_name = reid_config.get(
                "reid_model_name", "osnet_x1_0_market1501"
            )
            self.reid_similarity_threshold = reid_config.get(
                "reid_similarity_threshold", 0.65
            )
            if self.REID_MODEL_PATH:
                print(
                    f"[PlayerTracker] Initializing ReIDModel with weights: {self.REID_MODEL_PATH} and arch: {reid_model_name}"
                )
                self.reid_model_instance = ReIDModel(
                    model_name=reid_model_name,
                    device=self.device,
                    reid_model_path=self.REID_MODEL_PATH,
                )
            else:
                print(
                    f"[PlayerTracker] Initializing ReIDModel with hub model: {reid_model_name} (no specific weights path provided)"
                )
                self.reid_model_instance = ReIDModel(
                    model_name=reid_model_name, device=self.device
                )
        else:
            print(
                "[PlayerTracker] No ReID configuration provided. ReID will use defaults or be inactive if model fails to load."
            )
            self.reid_model_instance = ReIDModel(device=self.device)  # Default init

    def _get_player_patch(self, frame, bbox):
        x1, y1, x2, y2 = map(int, bbox)
        # Ensure coordinates are within frame boundaries
        h, w = frame.shape[:2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w - 1, x2), min(h - 1, y2)
        if x1 >= x2 or y1 >= y2:
            return np.array([])  # Return empty if bbox is invalid
        patch = frame[y1:y2, x1:x2]
        return patch

    def update(
        self, frame, frame_num=None, current_homography=None, person_min_score=0.5
    ):
        """
        Detects and tracks players. Implements HSV for continuous tracking and Re-ID for recovery.
        """
        tracked_players_output = (
            []
        )  # List of (bbox, bottom_center_point, track_id) from YOLO/BoT-SORT

        # Run YOLO detection with tracking
        results = self.yolo_model.track(
            frame,
            persist=True,
            tracker="botsort.yaml",
            verbose=False,
            classes=[0],  # Person class
            conf=person_min_score,
        )

        current_track_ids_in_frame = set()
        raw_detections_map = {}  # {track_id: (bbox, center)}

        if results and results[0].boxes is not None and results[0].boxes.id is not None:
            boxes_xyxy = results[0].boxes.xyxy.cpu().numpy()
            track_ids_from_yolo = results[0].boxes.id.int().cpu().tolist()

            for i, tid in enumerate(track_ids_from_yolo):
                bbox_xyxy = boxes_xyxy[i]
                x1, y1, x2, y2 = map(int, bbox_xyxy)

                if x2 > x1 and y2 > y1:  # Valid bbox
                    bbox = [x1, y1, x2, y2]
                    cx = int((x1 + x2) / 2)
                    cy = int(y2)  # Bottom center for player
                    center = (cx, cy)

                    tracked_players_output.append((bbox, center, tid))
                    current_track_ids_in_frame.add(tid)
                    raw_detections_map[tid] = (bbox, center)

        # --- Hybrid Re-ID and HSV Logic ---
        # Identify which roles ("Player 1", "Player 2") are currently "active" (i.e., their track_id is in current_track_ids_in_frame)
        active_roles = set()
        for role, tid in self.role_to_track_id.items():
            if tid in current_track_ids_in_frame:
                active_roles.add(role)
                self.player_lost_frames_count[role] = (
                    0  # Reset lost count if player is seen
                )
            else:
                # Increment lost count only if the role was previously assigned a track_id
                if tid is not None:  # Check if role_to_track_id[role] was ever assigned
                    self.player_lost_frames_count[role] += 1

        # Attempt Re-ID for "lost" players
        unassigned_track_ids = current_track_ids_in_frame - set(
            self.role_to_track_id.values()
        )

        # Prioritize re-identifying roles that have initial features
        roles_to_reid = []
        if (
            "Player 1" not in active_roles
            and self.initial_player_reid_features["Player 1"] is not None
            and self.player_lost_frames_count["Player 1"] < self.MAX_FRAMES_PLAYER_LOST
        ):
            roles_to_reid.append("Player 1")
        if (
            "Player 2" not in active_roles
            and self.initial_player_reid_features["Player 2"] is not None
            and self.player_lost_frames_count["Player 2"] < self.MAX_FRAMES_PLAYER_LOST
        ):
            roles_to_reid.append("Player 2")

        if (
            self.reid_model_instance
            and self.reid_model_instance.model
            and unassigned_track_ids
            and roles_to_reid
        ):
            # print(f"[Frame {frame_num}] Attempting Re-ID for lost roles: {roles_to_reid} from unassigned tracks: {unassigned_track_ids}")

            # Store features for all unassigned tracks first
            unassigned_track_features = {}
            for new_tid in unassigned_track_ids:
                bbox_new, _ = raw_detections_map[new_tid]
                patch_new = self._get_player_patch(frame, bbox_new)
                if patch_new.size > 0:
                    unassigned_track_features[new_tid] = (
                        self.reid_model_instance.extract_features(patch_new)
                    )

            reid_assignments_this_frame = (
                {}
            )  # To avoid assigning multiple roles to the same new_tid or one role to multiple new_tids

            for role_to_find in roles_to_reid:
                # initial_features_lost_player is a LIST of ndarrays
                list_of_initial_features_for_role = (
                    self.initial_player_reid_features.get(role_to_find, [])
                )

                if not list_of_initial_features_for_role:  # Check if the list is empty
                    # print(f"[PlayerTracker Update] No initial Re-ID features stored for {role_to_find}. Skipping Re-ID for this role.")
                    continue

                best_match_tid_for_role = None
                highest_similarity_for_role = -1

                for new_tid, current_new_features in unassigned_track_features.items():
                    if (
                        new_tid in reid_assignments_this_frame.values()
                    ):  # Already assigned to another role in this frame
                        continue
                    if current_new_features is None:
                        continue

                    max_similarity_for_this_new_track = -1

                    for (
                        ref_feature
                    ) in (
                        list_of_initial_features_for_role
                    ):  # Iterate through each stored feature for the role
                        if not isinstance(ref_feature, np.ndarray):
                            print(
                                f"[PlayerTracker Update] Warning: Stored reference feature for {role_to_find} is not an ndarray. Type: {type(ref_feature)}. Skipping this ref feature."
                            )
                            continue
                        try:
                            similarity = F.cosine_similarity(
                                torch.from_numpy(ref_feature).unsqueeze(0),
                                torch.from_numpy(current_new_features).unsqueeze(0),
                            ).item()
                            if similarity > max_similarity_for_this_new_track:
                                max_similarity_for_this_new_track = similarity
                        except Exception as e_sim:
                            print(
                                f"[PlayerTracker Update] Cosine similarity error during Re-ID for {role_to_find} with new_tid {new_tid} (ref vs current): {e_sim}"
                            )
                            # continue to next ref_feature or new_tid based on desired error handling

                    # Check if this new_tid is a better match for the role_to_find than previously checked new_tids
                    if max_similarity_for_this_new_track > highest_similarity_for_role:
                        highest_similarity_for_role = max_similarity_for_this_new_track
                        best_match_tid_for_role = new_tid

                # After checking all new_tids against all ref_features for role_to_find:
                if (
                    best_match_tid_for_role is not None
                    and highest_similarity_for_role >= self.reid_similarity_threshold
                ):
                    if (
                        role_to_find
                        not in reid_assignments_this_frame  # Ensure this role hasn't been assigned yet this frame
                        # and best_match_tid_for_role not in reid_assignments_this_frame.values() # Ensure tid not taken by another role (already handled by inner check)
                    ):
                        print(
                            f"[Frame {frame_num}] Re-ID SUCCESS: {role_to_find} (lost) re-identified as new TrackID {best_match_tid_for_role} with similarity {highest_similarity_for_role:.2f}"
                        )

                        self.role_to_track_id[role_to_find] = best_match_tid_for_role
                        reid_assignments_this_frame[role_to_find] = (
                            best_match_tid_for_role
                        )

                        bbox_reid, _ = raw_detections_map[best_match_tid_for_role]
                        hsv_hist_reid = self.extract_player_hsv_histogram(
                            frame, bbox_reid
                        )
                        if hsv_hist_reid is not None:
                            self.player_appearance[role_to_find].append(
                                hsv_hist_reid
                            )  # Append to list
                            print(
                                f"  Updated HSV model for {role_to_find} based on TrackID {best_match_tid_for_role}."
                            )

                        self.player_lost_frames_count[role_to_find] = 0
                        active_roles.add(role_to_find)
                        if best_match_tid_for_role in unassigned_track_features:
                            del unassigned_track_features[best_match_tid_for_role]
                        if best_match_tid_for_role in unassigned_track_ids:
                            unassigned_track_ids.remove(best_match_tid_for_role)
                    # else:
                    # print(f"[Frame {frame_num}] Re-ID Conflict or already assigned: Role {role_to_find} / TID {best_match_tid_for_role}")
                # else:
                # print(f"[Frame {frame_num}] Re-ID FAILED for {role_to_find}: No new track found with sufficient similarity (best was {highest_similarity_for_role:.2f} for TID {best_match_tid_for_role}). Lost count: {self.player_lost_frames_count[role_to_find]}")

        # If roles are still not active after Re-ID attempt, and they've been lost for too long, clear their track_id
        # This allows assign_initial_player_roles or HSV reassign to potentially pick them up later if they reappear distinctly
        for role_name in ["Player 1", "Player 2"]:
            if (
                role_name not in active_roles
                and self.player_lost_frames_count[role_name]
                >= self.MAX_FRAMES_PLAYER_LOST
            ):
                if self.role_to_track_id.get(role_name) is not None:
                    print(
                        f"[Frame {frame_num}] Player {role_name} (TrackID {self.role_to_track_id[role_name]}) truly lost after {self.player_lost_frames_count[role_name]} frames. Clearing their current track ID."
                    )
                    self.role_to_track_id[role_name] = None
                    # self.player_appearance[role_name] = None # Optionally clear HSV too, or keep last known
                    # Keep initial_player_reid_features intact for future Re-ID attempts

        return tracked_players_output  # Return the raw YOLO detections

    def extract_player_hsv_histogram(self, frame, bbox):
        x1, y1, x2, y2 = bbox
        roi = frame[y1:y2, x1:x2]
        if roi.size == 0:
            return None
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        hist = cv2.calcHist([hsv], [0, 1], None, [30, 32], [0, 180, 0, 256])
        cv2.normalize(hist, hist)
        return hist.flatten()

    def prime_with_multiple_user_selections(
        self, player1_patches_and_frames, player2_patches_and_frames
    ):
        print("[PlayerTracker] Priming with multiple user selections.")
        self.initial_player_reid_features["Player 1"] = []
        self.player_appearance["Player 1"] = []
        self.initial_player_reid_features["Player 2"] = []
        self.player_appearance["Player 2"] = []

        if self.reid_model_instance and self.reid_model_instance.model:
            # Player 1
            for item in player1_patches_and_frames:
                frame_img, bbox = None, None
                if isinstance(item, tuple) and len(item) == 2:  # Expected (frame, bbox)
                    frame_img, bbox = item
                elif hasattr(item, "shape"):  # Assume it's a patch
                    # This case is harder to handle for HSV without original frame, focusing on (frame, bbox)
                    print(
                        "  [Warning] Direct patch priming for HSV not fully supported, skipping HSV for this item if it's a patch."
                    )
                    patch = item
                    if patch.size > 0:
                        reid_features = self.reid_model_instance.extract_features(patch)
                        self.initial_player_reid_features["Player 1"].append(
                            reid_features
                        )
                    continue  # Skip HSV part for direct patch

                if frame_img is not None and bbox is not None:
                    patch = self._get_player_patch(frame_img, bbox)
                    if patch.size > 0:
                        reid_features = self.reid_model_instance.extract_features(patch)
                        self.initial_player_reid_features["Player 1"].append(
                            reid_features
                        )
                        hsv_hist = self.extract_player_hsv_histogram(frame_img, bbox)
                        if hsv_hist is not None:
                            self.player_appearance["Player 1"].append(hsv_hist)
            print(
                f"  Stored {len(self.initial_player_reid_features['Player 1'])} ReID features and {len(self.player_appearance['Player 1'])} HSV histograms for Player 1."
            )

            # Player 2
            for item in player2_patches_and_frames:
                frame_img, bbox = None, None
                if isinstance(item, tuple) and len(item) == 2:
                    frame_img, bbox = item
                elif hasattr(item, "shape"):
                    print(
                        "  [Warning] Direct patch priming for HSV not fully supported, skipping HSV for this item if it's a patch."
                    )
                    patch = item
                    if patch.size > 0:
                        reid_features = self.reid_model_instance.extract_features(patch)
                        self.initial_player_reid_features["Player 2"].append(
                            reid_features
                        )
                    continue

                if frame_img is not None and bbox is not None:
                    patch = self._get_player_patch(frame_img, bbox)
                    if patch.size > 0:
                        reid_features = self.reid_model_instance.extract_features(patch)
                        self.initial_player_reid_features["Player 2"].append(
                            reid_features
                        )
                        hsv_hist = self.extract_player_hsv_histogram(frame_img, bbox)
                        if hsv_hist is not None:
                            self.player_appearance["Player 2"].append(hsv_hist)
            print(
                f"  Stored {len(self.initial_player_reid_features['Player 2'])} ReID features and {len(self.player_appearance['Player 2'])} HSV histograms for Player 2."
            )
        else:
            print(
                "[PlayerTracker] ReID model not available. Cannot prime with user selection features."
            )

    def assign_initial_player_roles(self, tracked_players, frame):
        # Check if features were primed by user (i.e., if the lists are non-empty)
        user_primed_p1 = bool(self.initial_player_reid_features.get("Player 1"))
        user_primed_p2 = bool(self.initial_player_reid_features.get("Player 2"))

        assigned_p1 = self.role_to_track_id.get("Player 1") is not None
        assigned_p2 = self.role_to_track_id.get("Player 2") is not None

        if user_primed_p1 and user_primed_p2 and not (assigned_p1 and assigned_p2):
            print(
                "[PlayerTracker] Attempting to assign roles based on user-primed features (multiple)."
            )

            # Create a list of (track_id, current_features, bbox) for all current tracks
            live_tracks_data = []
            for bbox_live, center_live, track_id_live in tracked_players:
                if not (self.reid_model_instance and self.reid_model_instance.model):
                    print("  ReID model not available for matching.")
                    break
                patch_live = self._get_player_patch(frame, bbox_live)
                if patch_live.size == 0:
                    continue
                current_features_live = self.reid_model_instance.extract_features(
                    patch_live
                )
                live_tracks_data.append(
                    (track_id_live, current_features_live, bbox_live)
                )

            # Try to assign Player 1
            if not assigned_p1:
                best_match_tid_p1 = None
                highest_similarity_p1 = -1
                best_match_bbox_p1 = None

                for track_id_live, current_features_live, bbox_live in live_tracks_data:
                    for ref_feature_p1 in self.initial_player_reid_features["Player 1"]:
                        try:
                            similarity = F.cosine_similarity(
                                torch.from_numpy(ref_feature_p1).unsqueeze(0),
                                torch.from_numpy(current_features_live).unsqueeze(0),
                            ).item()
                            if similarity > highest_similarity_p1:
                                highest_similarity_p1 = similarity
                                best_match_tid_p1 = track_id_live
                                best_match_bbox_p1 = bbox_live
                        except Exception as e:
                            print(
                                f"  Error comparing features for Player 1 (TrackID {track_id_live}): {e}"
                            )

                if (
                    best_match_tid_p1
                    and highest_similarity_p1 >= self.reid_similarity_threshold
                ):
                    # Check if this tid is already assigned to Player 2
                    if self.role_to_track_id.get("Player 2") != best_match_tid_p1:
                        self.role_to_track_id["Player 1"] = best_match_tid_p1
                        # Optionally update current appearance based on this match
                        hsv_hist = self.extract_player_hsv_histogram(
                            frame, best_match_bbox_p1
                        )
                        if hsv_hist is not None:
                            # If player_appearance for P1 is empty or needs update
                            if not self.player_appearance[
                                "Player 1"
                            ]:  # Or some other logic
                                self.player_appearance["Player 1"].append(hsv_hist)
                        print(
                            f"  Assigned Player 1 to TrackID {best_match_tid_p1} (Max Similarity: {highest_similarity_p1:.2f})."
                        )
                        assigned_p1 = True
                        if best_match_tid_p1 in self.generic_player_labels:
                            del self.generic_player_labels[best_match_tid_p1]

            # Try to assign Player 2
            if not assigned_p2:
                best_match_tid_p2 = None
                highest_similarity_p2 = -1
                best_match_bbox_p2 = None

                for track_id_live, current_features_live, bbox_live in live_tracks_data:
                    # Ensure this track_id is not already Player 1
                    if self.role_to_track_id.get("Player 1") == track_id_live:
                        continue
                    for ref_feature_p2 in self.initial_player_reid_features["Player 2"]:
                        try:
                            similarity = F.cosine_similarity(
                                torch.from_numpy(ref_feature_p2).unsqueeze(0),
                                torch.from_numpy(current_features_live).unsqueeze(0),
                            ).item()
                            if similarity > highest_similarity_p2:
                                highest_similarity_p2 = similarity
                                best_match_tid_p2 = track_id_live
                                best_match_bbox_p2 = bbox_live
                        except Exception as e:
                            print(
                                f"  Error comparing features for Player 2 (TrackID {track_id_live}): {e}"
                            )

                if (
                    best_match_tid_p2
                    and highest_similarity_p2 >= self.reid_similarity_threshold
                ):
                    self.role_to_track_id["Player 2"] = best_match_tid_p2
                    hsv_hist = self.extract_player_hsv_histogram(
                        frame, best_match_bbox_p2
                    )
                    if hsv_hist is not None:
                        if not self.player_appearance["Player 2"]:
                            self.player_appearance["Player 2"].append(hsv_hist)
                    print(
                        f"  Assigned Player 2 to TrackID {best_match_tid_p2} (Max Similarity: {highest_similarity_p2:.2f})."
                    )
                    assigned_p2 = True
                    if best_match_tid_p2 in self.generic_player_labels:
                        del self.generic_player_labels[best_match_tid_p2]

            if assigned_p1 and assigned_p2:
                print(
                    "[PlayerTracker] Both Player 1 and Player 2 assigned from user-primed features."
                )
            elif not (assigned_p1 and assigned_p2):
                print(
                    "[PlayerTracker] Could not assign one or both roles from user-primed features in this frame. Will retry or may need fallback."
                )

        elif not (assigned_p1 and assigned_p2):
            print(
                "[PlayerTracker] User priming skipped, failed, or not enough data yet. Using original automatic role assignment logic."
            )
            # Fallback to original behavior if not primed or if priming failed to assign

    def prime_with_user_selection(self, selection_frame, p1_bbox, p2_bbox):
        # This method is deprecated and replaced by prime_with_multiple_user_selections
        # Kept for now to avoid breaking existing calls if any, but should be removed.
        print(
            "[PlayerTracker] WARNING: prime_with_user_selection is deprecated. Use prime_with_multiple_user_selections."
        )

        # For compatibility, we can adapt it to call the new method with single items
        # But it's better to update the calling code.
        # For now, let's make it a no-op or call the new method with single item lists

        p1_item = (
            (selection_frame, p1_bbox)
            if selection_frame is not None and p1_bbox is not None
            else None
        )
        p2_item = (
            (selection_frame, p2_bbox)
            if selection_frame is not None and p2_bbox is not None
            else None
        )

        p1_list = [p1_item] if p1_item else []
        p2_list = [p2_item] if p2_item else []

        self.prime_with_multiple_user_selections(p1_list, p2_list)

    def reassign_roles_after_cut(self, new_tracks, frame):
        """
        Reassigns player roles by matching new tracks to stored player appearances.
        """
        candidates = {}
        for bbox, center, tid in new_tracks:
            hist = self.extract_player_hsv_histogram(frame, bbox)
            if hist is not None:
                candidates[tid] = hist

        reassigned = {}
        used_tids = set()
        for role, prev_hist_list in self.player_appearance.items():
            if not prev_hist_list:  # No appearance model for this role
                continue

            # Use the average of stored histograms, or the first one if only one
            # For simplicity, let's use the first one for now, or average if multiple
            # This part might need refinement depending on how HSV matching against a list is best done.
            # For now, we'll match against the first stored HSV. A more robust way would be to average them
            # or find max similarity against any of them.
            # Let's assume for now we match against the first one, if available.
            if not prev_hist_list:
                continue

            # Simple approach: use the first available histogram as reference
            # More complex: average histograms or find best match across all stored histograms
            # For now, let's use the first one as a placeholder for more complex logic if needed.
            # Or, if we want to be more robust, we should consider how to best use a list of HSV appearances.
            # Perhaps averaging them is a good start if the list is not too diverse.

            # Let's try matching against each stored histogram and take the best overall score.
            best_tid_for_role = None
            best_score_for_role = float("inf")

            for (
                prev_hist_single
            ) in prev_hist_list:  # Iterate through stored HSV for this role
                current_best_tid_for_hist = None
                current_best_score_for_hist = float("inf")
                for tid, cand_hist in candidates.items():
                    if (
                        tid in used_tids
                    ):  # If this track ID is already assigned to another role
                        continue
                    score = cv2.compareHist(
                        prev_hist_single, cand_hist, cv2.HISTCMP_BHATTACHARYYA
                    )
                    if score < current_best_score_for_hist:
                        current_best_score_for_hist = score
                        current_best_tid_for_hist = tid

                # If this histogram provided a better match for the role than previous histograms for the same role
                if (
                    current_best_tid_for_hist is not None
                    and current_best_score_for_hist < best_score_for_role
                ):
                    best_score_for_role = current_best_score_for_hist
                    best_tid_for_role = current_best_tid_for_hist

            if best_tid_for_role is not None:
                reassigned[role] = best_tid_for_role
                used_tids.add(
                    best_tid_for_role
                )  # Mark this track ID as used for this frame's reassignment

        if len(reassigned) == 2:  # If both P1 and P2 were successfully reassigned
            self.role_to_track_id = reassigned
            print(
                f"[PlayerTracker] Reassigned roles after cut: {self.role_to_track_id}"
            )
        elif len(reassigned) == 1:  # If only one player was reassigned
            # This case is tricky. Do we update one and leave the other? Or clear the other?
            # For now, let's update the one that was found.
            for role, tid in reassigned.items():
                self.role_to_track_id[role] = tid
            print(
                f"[PlayerTracker] Partially reassigned roles after cut: {self.role_to_track_id}. Other role might be lost or unclear."
            )
        else:
            print(
                "[PlayerTracker] Failed to reassign roles after cut (0 players matched). Roles might be lost or appearances changed too much."
            )
            # Potentially clear self.role_to_track_id for P1 and P2 if reassign consistently fails
            # self.role_to_track_id["Player 1"] = None
            # self.role_to_track_id["Player 2"] = None

    def get_player_display_name(self, track_id):
        # This method primarily relies on self.role_to_track_id,
        # which is now updated by initial assignment, HSV reassign, and Re-ID fallback.
        for role, tid in self.role_to_track_id.items():
            if tid == track_id:  # and tid is not None
                return role

        # If the track_id is not P1 or P2, we do not assign a new generic ID.
        # The request was to only identify "Player 1" or "Player 2".
        # Tracks that are not P1 or P2 will not get a display name from this function.
        return None

    def get_player_side(self, center_point, homography, net_y_on_ref):
        pt = np.array(
            [[[float(center_point[0]), float(center_point[1])]]], dtype=np.float32
        )
        try:
            transformed = cv2.perspectiveTransform(pt, homography)
            return "top" if transformed[0, 0, 1] < net_y_on_ref else "bottom"
        except:
            return None

    def detect_role_switch(self):
        """
        If both tracked player roles have switched sides consistently for a few frames, swap their labels.
        """
        recent_sides = {role: [] for role in ["Player 1", "Player 2"]}
        for role, tid in self.role_to_track_id.items():
            if tid in self.track_history:
                for _, center in list(self.track_history[tid])[-10:]:  # Last 10 frames
                    side = self.get_player_side(
                        center, self.last_homography, self.center_line_y
                    )
                    if side:
                        recent_sides[role].append(side)

        if all(sides for sides in recent_sides.values()):
            roles_flipped = (
                recent_sides["Player 1"].count("bottom") > 7
                and recent_sides["Player 2"].count("top") > 7
            )
            if roles_flipped:
                print("[PlayerTracker] Detected side switch. Swapping roles.")
                self.role_to_track_id["Player 1"], self.role_to_track_id["Player 2"] = (
                    self.role_to_track_id["Player 2"],
                    self.role_to_track_id["Player 1"],
                )

    def get_player_display_name(self, track_id):
        for role, tid in self.role_to_track_id.items():
            if tid == track_id:
                return role

    def force_reassign_roles(self, current_ids):
        """
        In case tracking fails or IDs change dramatically mid-match.
        """
        if len(current_ids) >= 2:
            self.role_to_track_id["Player 1"] = current_ids[0]
            self.role_to_track_id["Player 2"] = current_ids[1]
            print(f"[PlayerTracker] Forced reassignment: {self.role_to_track_id}")


## Bounce Detection


class BounceDetector:
    def __init__(self, path_model=None):
        self.model = ctb.CatBoostRegressor()
        self.threshold = 0.45
        if path_model:
            self.load_model(path_model)

    def load_model(self, path_model):
        self.model.load_model(path_model)

    def prepare_features(self, x_ball, y_ball):
        labels = pd.DataFrame(
            {
                "frame": range(len(x_ball)),
                "x-coordinate": x_ball,
                "y-coordinate": y_ball,
            }
        )

        num = 3
        eps = 1e-15
        for i in range(1, num):
            labels["x_lag_{}".format(i)] = labels["x-coordinate"].shift(i)
            labels["x_lag_inv_{}".format(i)] = labels["x-coordinate"].shift(-i)
            labels["y_lag_{}".format(i)] = labels["y-coordinate"].shift(i)
            labels["y_lag_inv_{}".format(i)] = labels["y-coordinate"].shift(-i)
            labels["x_diff_{}".format(i)] = abs(
                labels["x_lag_{}".format(i)] - labels["x-coordinate"]
            )
            labels["y_diff_{}".format(i)] = (
                labels["y_lag_{}".format(i)] - labels["y-coordinate"]
            )
            labels["x_diff_inv_{}".format(i)] = abs(
                labels["x_lag_inv_{}".format(i)] - labels["x-coordinate"]
            )
            labels["y_diff_inv_{}".format(i)] = (
                labels["y_lag_inv_{}".format(i)] - labels["y-coordinate"]
            )
            labels["x_div_{}".format(i)] = abs(
                labels["x_diff_{}".format(i)]
                / (labels["x_diff_inv_{}".format(i)] + eps)
            )
            labels["y_div_{}".format(i)] = labels["y_diff_{}".format(i)] / (
                labels["y_diff_inv_{}".format(i)] + eps
            )

        for i in range(1, num):
            labels = labels[labels["x_lag_{}".format(i)].notna()]
            labels = labels[labels["x_lag_inv_{}".format(i)].notna()]
        labels = labels[labels["x-coordinate"].notna()]

        colnames_x = (
            ["x_diff_{}".format(i) for i in range(1, num)]
            + ["x_diff_inv_{}".format(i) for i in range(1, num)]
            + ["x_div_{}".format(i) for i in range(1, num)]
        )
        colnames_y = (
            ["y_diff_{}".format(i) for i in range(1, num)]
            + ["y_diff_inv_{}".format(i) for i in range(1, num)]
            + ["y_div_{}".format(i) for i in range(1, num)]
        )
        colnames = colnames_x + colnames_y

        features = labels[colnames]
        return features, list(labels["frame"])

    def predict(self, x_ball, y_ball, smooth=True):
        if smooth:
            x_ball, y_ball = self.smooth_predictions(x_ball, y_ball)
        features, num_frames = self.prepare_features(x_ball, y_ball)
        preds = self.model.predict(features)
        ind_bounce = np.where(preds > self.threshold)[0]
        if len(ind_bounce) > 0:
            ind_bounce = self.postprocess(ind_bounce, preds)
        frames_bounce = [num_frames[x] for x in ind_bounce]
        return set(frames_bounce)

    def smooth_predictions(self, x_ball, y_ball):
        is_none = [int(x is None) for x in x_ball]
        interp = 5
        counter = 0
        for num in range(interp, len(x_ball) - 1):
            if (
                not x_ball[num]
                and sum(is_none[num - interp : num]) == 0
                and counter < 3
            ):
                x_ext, y_ext = self.extrapolate(
                    x_ball[num - interp : num], y_ball[num - interp : num]
                )
                x_ball[num] = x_ext
                y_ball[num] = y_ext
                is_none[num] = 0
                if x_ball[num + 1]:
                    dist = distance.euclidean(
                        (x_ext, y_ext), (x_ball[num + 1], y_ball[num + 1])
                    )
                    if dist > 80:
                        x_ball[num + 1], y_ball[num + 1], is_none[num + 1] = (
                            None,
                            None,
                            1,
                        )
                counter += 1
            else:
                counter = 0
        return x_ball, y_ball

    def extrapolate(self, x_coords, y_coords):
        xs = list(range(len(x_coords)))
        func_x = CubicSpline(xs, x_coords, bc_type="natural")
        x_ext = func_x(len(x_coords))
        func_y = CubicSpline(xs, y_coords, bc_type="natural")
        y_ext = func_y(len(x_coords))
        return float(x_ext), float(y_ext)

    def postprocess(self, ind_bounce, preds):
        ind_bounce_filtered = [ind_bounce[0]]
        for i in range(1, len(ind_bounce)):
            if (ind_bounce[i] - ind_bounce[i - 1]) != 1:
                cur_ind = ind_bounce[i]
                ind_bounce_filtered.append(cur_ind)
            elif preds[ind_bounce[i]] > preds[ind_bounce[i - 1]]:
                ind_bounce_filtered[-1] = ind_bounce[i]
        return ind_bounce_filtered


keypoint_names = [
    "BTL",
    "BTR",
    "BBL",
    "BBR",
    "BTLI",
    "BBLI",
    "BTRI",
    "BBRI",
    "ITL",
    "ITR",
    "IBL",
    "IBR",
    "ITM",
    "IBM",
]

court_lines = [
    ("BTL", "BTLI"),
    ("BTLI", "BTRI"),
    ("BTRI", "BTR"),
    ("BTL", "BBL"),
    ("BTR", "BBR"),
    ("BBL", "BBLI"),
    ("BBLI", "BBRI"),
    ("BBLI", "IBL"),
    ("BBRI", "IBR"),
    ("BBRI", "BBR"),
    ("BTLI", "ITL"),
    ("BTRI", "ITR"),
    ("ITL", "ITM"),
    ("ITM", "IBM"),
    ("ITL", "IBL"),
    ("ITR", "IBR"),
    ("IBL", "IBM"),
    ("IBM", "IBR"),
    ("ITM", "ITR"),
]


def ensure_720p(input_path, intermediate_path):
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()

    print(f"Original input: {width}x{height}, fps={fps:.2f}")
    if (width != 1280) or (height != 720):
        print(f"Resizing from ({width}x{height}) to (1280x720) -> {intermediate_path}")
        cap_in = cv2.VideoCapture(input_path)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(intermediate_path, fourcc, fps, (1280, 720))

        while True:
            ret, frame = cap_in.read()
            if not ret:
                break
            frame = cv2.resize(frame, (1280, 720), interpolation=cv2.INTER_AREA)
            out.write(frame)

        cap_in.release()
        out.release()
        print(f"Finished writing intermediate: {intermediate_path}")
        return intermediate_path
    else:
        print("Video is already 1280x720; using input directly.")
        return input_path


def get_court_img():
    """Build a 720p-like minimap with white lines on black background."""
    court_ref = CourtReference()
    court = court_ref.build_court_reference()
    court = cv2.dilate(court, np.ones((10, 10), dtype=np.uint8))
    court_img = (np.stack((court, court, court), axis=2) * 255).astype(np.uint8)
    return court_img


def draw_court_keypoints_and_lines(frame, kps, frame_width, frame_height):
    """
    Draw tennis court lines (green) and keypoints (red) on 'frame'.
    """
    for start_name, end_name in court_lines:
        try:
            s_idx = keypoint_names.index(start_name)
            e_idx = keypoint_names.index(end_name)
            if kps[s_idx] is None or kps[e_idx] is None:
                continue
            x1 = int(kps[s_idx][0, 0])
            y1 = int(kps[s_idx][0, 1])
            x2 = int(kps[e_idx][0, 0])
            y2 = int(kps[e_idx][0, 1])
            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        except ValueError:
            pass

    # Keypoints
    for i, pt in enumerate(kps):
        if pt is None:
            continue
        x = int(pt[0, 0])
        y = int(pt[0, 1])
        cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)
        label = keypoint_names[i]
        (tw, th), base = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
        cv2.rectangle(
            frame, (x - 5, y - th - 5), (x - 5 + tw, y - 5), (255, 255, 255), -1
        )
        cv2.putText(
            frame, label, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1
        )


def write(imgs_res, fps, output_path):
    if not imgs_res:
        print("No frames, skipping write.")
        return
    H, W = imgs_res[0].shape[:2]
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))
    for frame in imgs_res:
        out.write(frame)
    out.release()
    print(f"[write] Finished writing {output_path}")


def add_bounces_to_minimap_video(
    original_minimap_path,
    output_minimap_path_final,  # Path for the final minimap with bounces
    temp_bounce_minimap_path,  # Temporary path for writing minimap with bounces
    fps,
    minimap_width,
    minimap_height,
    bounces_set,  # The set of bounce frame numbers
    ball_track_list,  # Full list of ball positions
    homography_matrices_list,  # Full list of inverse homographies
    ref_court_width,  # Width of the reference court image used for initial projection
    ref_court_height,  # Height of the reference court image used for initial projection
    bounce_color_tuple=(0, 255, 255),
    bounce_radius=10,
    bounce_thickness=-1,
):
    print(
        f"Attempting to add bounces to minimap video: {original_minimap_path} -> {temp_bounce_minimap_path}"
    )
    print(f"[DEBUG] add_bounces_to_minimap_video ENTERED:")
    print(f"  original_minimap_path: {original_minimap_path}")
    print(f"  output_minimap_path_final: {output_minimap_path_final}")
    print(f"  temp_bounce_minimap_path: {temp_bounce_minimap_path}")
    print(
        f"  fps: {fps}, minimap_width: {minimap_width}, minimap_height: {minimap_height}"
    )
    print(
        f"  len(bounces_set): {len(bounces_set)}, bounces_set: {bounces_set if len(bounces_set) < 20 else str(list(bounces_set)[:20]) + '...'}"
    )
    print(f"  len(ball_track_list): {len(ball_track_list)}")
    print(f"  len(homography_matrices_list): {len(homography_matrices_list)}")
    print(f"  ref_court_width: {ref_court_width}, ref_court_height: {ref_court_height}")

    cap_minimap = cv2.VideoCapture(original_minimap_path)
    if not cap_minimap.isOpened():
        print(
            f"Error: Could not open minimap video for bounce addition: {original_minimap_path}"
        )
        return False

    temp_dir = os.path.dirname(temp_bounce_minimap_path)
    if temp_dir and not os.path.exists(temp_dir):
        os.makedirs(temp_dir, exist_ok=True)

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out_minimap_with_bounces = cv2.VideoWriter(
        temp_bounce_minimap_path, fourcc, float(fps), (minimap_width, minimap_height)
    )

    if not out_minimap_with_bounces.isOpened():
        print(
            f"Error: Could not open temporary minimap video for writing: {temp_bounce_minimap_path}"
        )
        cap_minimap.release()
        return False

    frame_idx = 0
    total_frames_minimap = int(cap_minimap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"Processing {total_frames_minimap} frames for bounce overlay on minimap...")
    try:
        pbar_bounce = tqdm(total=total_frames_minimap, desc="Adding Bounces to Minimap")
    except NameError:  # If tqdm is not available
        pbar_bounce = None
        print("tqdm not found, proceeding without progress bar for bounce overlay.")

    while True:
        ret, minimap_frame = cap_minimap.read()
        if not ret:
            print(
                f"[DEBUG] add_bounces_to_minimap_video: End of minimap frames at frame_idx {frame_idx}."
            )
            break

        minimap_frame_copy = minimap_frame.copy()

        current_homography = None
        if frame_idx < len(homography_matrices_list):
            current_homography = homography_matrices_list[frame_idx]

        if frame_idx % 30 == 0:  # Print every 30 frames to reduce verbosity
            print(
                f"[DEBUG] add_bounces: frame_idx {frame_idx}, current_homography is {'None' if current_homography is None else 'Valid'}"
            )

        if current_homography is not None:
            for bounce_frame_num in bounces_set:
                if bounce_frame_num <= frame_idx:
                    ball_pos_at_bounce = None
                    if bounce_frame_num < len(ball_track_list):
                        ball_pos_at_bounce = ball_track_list[bounce_frame_num]

                    if (
                        ball_pos_at_bounce
                        and ball_pos_at_bounce[0] is not None
                        and ball_pos_at_bounce[1] is not None
                    ):
                        bpx, bpy = ball_pos_at_bounce
                        pt_to_transform = np.array(
                            [[[float(bpx), float(bpy)]]], dtype=np.float32
                        )
                        if (
                            frame_idx % 10 == 0 or bounce_frame_num == frame_idx
                        ):  # reduce verbosity, but print if bounce is current
                            print(
                                f"  [DEBUG] Drawing bounce: bounce_frame_num {bounce_frame_num} on minimap_frame_idx {frame_idx}"
                            )
                            print(
                                f"    ball_pos_at_bounce ({bounce_frame_num}): ({bpx}, {bpy})"
                            )
                        try:
                            mapped_bounce = cv2.perspectiveTransform(
                                pt_to_transform, current_homography
                            )
                            if mapped_bounce is not None:
                                mx_bounce = int(
                                    mapped_bounce[0, 0, 0]
                                )  # This is on the large reference court scale
                                my_bounce = int(
                                    mapped_bounce[0, 0, 1]
                                )  # This is on the large reference court scale

                                # Scale bounce coordinates from reference court to minimap size
                                # Ensure ref_court_width and ref_court_height are not zero to avoid division by zero
                                if ref_court_width == 0 or ref_court_height == 0:
                                    # This case should ideally not happen if REF_COURT_WIDTH/HEIGHT are correctly obtained
                                    # print(f"Warning: ref_court_width ({ref_court_width}) or ref_court_height ({ref_court_height}) is zero. Skipping bounce scaling.")
                                    scaled_mx_bounce = mx_bounce
                                    scaled_my_bounce = my_bounce
                                else:
                                    scaled_mx_bounce = int(
                                        mx_bounce * minimap_width / ref_court_width
                                    )
                                    scaled_my_bounce = int(
                                        my_bounce * minimap_height / ref_court_height
                                    )

                                if (
                                    frame_idx % 10 == 0 or bounce_frame_num == frame_idx
                                ):  # reduce verbosity
                                    print(
                                        f"    Unscaled bounce on ref court: ({mx_bounce}, {my_bounce})"
                                    )
                                    print(
                                        f"    Scaled bounce on minimap: ({scaled_mx_bounce}, {scaled_my_bounce})"
                                    )

                                # Draw if within minimap bounds using scaled coordinates
                                if (
                                    0 <= scaled_mx_bounce < minimap_width
                                    and 0 <= scaled_my_bounce < minimap_height
                                ):
                                    cv2.circle(
                                        minimap_frame_copy,  # Draw on the frame read from the input minimap video
                                        (
                                            scaled_mx_bounce,
                                            scaled_my_bounce,
                                        ),  # Use scaled coords
                                        bounce_radius,
                                        bounce_color_tuple,
                                        bounce_thickness,
                                    )
                        except cv2.error:
                            if frame_idx % 10 == 0 or bounce_frame_num == frame_idx:
                                print(
                                    f"    [DEBUG] cv2.error during perspectiveTransform for bounce {bounce_frame_num} on minimap_frame {frame_idx}"
                                )
                            pass  # Ignore perspective transform errors

        out_minimap_with_bounces.write(minimap_frame_copy)
        if pbar_bounce:
            pbar_bounce.update(1)
        frame_idx += 1

    if pbar_bounce:
        pbar_bounce.close()

    cap_minimap.release()
    out_minimap_with_bounces.release()

    try:
        final_dir = os.path.dirname(output_minimap_path_final)
        if final_dir and not os.path.exists(final_dir):
            os.makedirs(final_dir, exist_ok=True)
        if os.path.exists(temp_bounce_minimap_path):
            os.replace(temp_bounce_minimap_path, output_minimap_path_final)
            print(
                f"Successfully added bounces. Final minimap video: {output_minimap_path_final}"
            )
            print(
                f"[DEBUG] os.replace successful: {temp_bounce_minimap_path} -> {output_minimap_path_final}"
            )
        else:
            print(
                f"Error: Temporary minimap video not found at {temp_bounce_minimap_path}"
            )
            print(f"[DEBUG] os.replace FAILED: {temp_bounce_minimap_path} not found.")
            return False
    except OSError as e:
        print(
            f"Error replacing original minimap with bounced version: {e}. Bounced minimap is at {temp_bounce_minimap_path}"
        )
        print(f"[DEBUG] os.replace FAILED with OSError: {e}")
        return False

    return True


if __name__ == "__main__":
    # --- Define Model Paths & Input/Output Paths (Update as needed) ---
    path_ball_track_model = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/models/ball_detection_weights/tracknet_weights.pt"
    path_court_model = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/models/court_detection_weights/model_tennis_court_det.pt"
    path_bounce_model = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/models/bounce_detection_weights/bounce_detection_weights.cbm"

    path_input_video = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/input_video/UCDwten/2/hawaii_2_edited.mp4"
    path_intermediate_video = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/input_video/hawaii_2_video.mp4"  # Intermediate file if resizing needed
    path_output_video = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/output/video/hawaii_2.mp4"
    path_minimap_video = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/output/video/hawaii_2_minimap.mp4"  # Separate minimap path
    path_output_bounce_heatmap = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/output/heatmaps/hawaii_2_bounce.png"
    path_output_player_heatmap = "/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/end-of-year-showcase-2025/output/heatmaps/hawaii_2_player.png"

    # Get reference court dimensions for minimap scaling (keep this here as it uses get_court_img())
    _temp_court_ref_img_for_dims = (
        get_court_img()
    )  # get_court_img returns the large reference court image
    REF_COURT_HEIGHT, REF_COURT_WIDTH = _temp_court_ref_img_for_dims.shape[:2]
    del _temp_court_ref_img_for_dims  # Free memory

    light_blue = (255, 255, 0)
    bounce_color_og = (0, 255, 255)  # OG bounce color for minimap
    # bounce_color = (0, 255, 255) # Now handled by add_bounces_to_minimap_video defaults
    box_color = (255, 0, 0)
    player_minimap_color = (255, 0, 0)
    player_minimap_radius = 20  # Smaller radius for players on minimap

    # --- Initialization ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    start_time = time.time()  # Overall script start time

    # 1) Scale to 720p if needed
    time_scaling_start = time.time()
    final_input = ensure_720p(path_input_video, path_intermediate_video)
    time_scaling_end = time.time()

    cap_check_total_frames = cv2.VideoCapture(final_input)
    if not cap_check_total_frames.isOpened():
        print(f"Error: Could not open video {final_input} to get total frame count.")
        sys.exit()
    total_frames_for_selection = int(
        cap_check_total_frames.get(cv2.CAP_PROP_FRAME_COUNT)
    )
    fps_for_selection = cap_check_total_frames.get(cv2.CAP_PROP_FPS)
    cap_check_total_frames.release()
    # Fallback fps if cap.get(cv2.CAP_PROP_FPS) returns 0 or invalid
    if not fps_for_selection or fps_for_selection <= 0:
        fps_for_selection = 30.0  # Default if needed

    print(f"Video has {total_frames_for_selection} total frames.")

    ball_detector = BallDetector(path_ball_track_model, device)
    court_detector = CourtDetectorNet(path_court_model, device)
    player_tracker_instance = PlayerTracker(device=device)  # New PlayerTracker
    bounce_detector = BounceDetector(path_bounce_model if DETECT_BOUNCES else None)

    # Helper function for Colab display (re-added)
    def display_frame_with_detections_colab(frame_img, detections, frame_label):
        # Detections is a list of (bbox_xyxy, temp_id)
        display_img = frame_img.copy()
        for i, (bbox, temp_id) in enumerate(detections):
            x1, y1, x2, y2 = map(int, bbox)
            cv2.rectangle(display_img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(
                display_img,
                f"ID: {temp_id}",
                (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (0, 255, 0),
                2,
            )

        print(f"\\n--- {frame_label} ---")
        # Convert BGR to RGB for matplotlib
        plt.figure(figsize=(12, 7))  # Adjusted figure size
        plt.imshow(cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB))
        plt.axis("off")
        plt.show()

    # --- New User-assisted Player Designation System ---
    MIN_CONFIRMATIONS = 5
    player1_confirmed_patches = []  # List of (frame, bbox) tuples
    player2_confirmed_patches = []  # List of (frame, bbox) tuples
    user_skipped_designation_entirely = False

    try:
        enable_user_selection_main = (
            input("Enable multi-frame user-assisted player selection? (yes/no): ")
            .strip()
            .lower()
        )
        if enable_user_selection_main == "yes":
            # Ensure YOLO model for selection is loaded (as per original snippet)
            yolo_model_for_selection_path = "yolov8n.pt"
            print(f"Loading YOLO model for selection: {yolo_model_for_selection_path}")
            yolo_model_for_selection = YOLO(yolo_model_for_selection_path)
            yolo_model_for_selection.to(device)

            shown_frame_indices = set()

            while (
                len(player1_confirmed_patches) < MIN_CONFIRMATIONS
                or len(player2_confirmed_patches) < MIN_CONFIRMATIONS
            ):
                print(f"\n--- Player Designation Round ---")
                print(
                    f"Player 1 Confirmations: {len(player1_confirmed_patches)}/{MIN_CONFIRMATIONS}"
                )
                print(
                    f"Player 2 Confirmations: {len(player2_confirmed_patches)}/{MIN_CONFIRMATIONS}"
                )
                if (
                    len(player1_confirmed_patches) >= MIN_CONFIRMATIONS
                    and len(player2_confirmed_patches) >= MIN_CONFIRMATIONS
                ):
                    print("Minimum confirmations met for both players.")
                    break

                cap_select = cv2.VideoCapture(final_input)
                if not cap_select.isOpened():
                    print(
                        f"Error: Could not open video {final_input} for player selection."
                    )
                    user_skipped_designation_entirely = True
                    break

                total_frames_for_selection_loop = int(
                    cap_select.get(cv2.CAP_PROP_FRAME_COUNT)
                )
                if total_frames_for_selection_loop == 0:
                    print("Error: Total frames is 0 for selection loop.")
                    user_skipped_designation_entirely = True
                    cap_select.release()
                    break

                num_candidate_frames_per_round = 3  # e.g., 3 to 5
                candidate_frames_indices_this_round = []

                # Try to get distinct frames
                possible_indices = list(
                    set(range(total_frames_for_selection_loop)) - shown_frame_indices
                )
                if len(possible_indices) >= num_candidate_frames_per_round:
                    candidate_frames_indices_this_round = sorted(
                        random.sample(possible_indices, num_candidate_frames_per_round)
                    )
                elif (
                    possible_indices
                ):  # If less than desired but some distinct frames are available
                    candidate_frames_indices_this_round = sorted(
                        random.sample(possible_indices, len(possible_indices))
                    )
                elif (
                    total_frames_for_selection_loop > 0
                ):  # If no distinct frames left, resample from all (allow repeats)
                    print(
                        "Warning: Re-sampling frames as all distinct ones have been shown or few are available."
                    )
                    candidate_frames_indices_this_round = sorted(
                        random.sample(
                            range(total_frames_for_selection_loop),
                            min(
                                num_candidate_frames_per_round,
                                total_frames_for_selection_loop,
                            ),
                        )
                    )
                else:
                    print("No frames to select from.")
                    user_skipped_designation_entirely = True
                    cap_select.release()
                    break

                shown_frame_indices.update(candidate_frames_indices_this_round)

                print(
                    f"Presenting candidate frames for this round: {candidate_frames_indices_this_round}"
                )

                # --- Step 1: Display all frames for the current round ---
                frames_data_for_input_round = (
                    []
                )  # Stores (frame_copy, detections, frame_idx)
                for frame_idx_select in candidate_frames_indices_this_round:
                    cap_select.set(cv2.CAP_PROP_POS_FRAMES, frame_idx_select)
                    ret_select, frame_cand_select = cap_select.read()
                    if not ret_select:
                        print(f"Could not read candidate frame {frame_idx_select}")
                        continue

                    results_select = yolo_model_for_selection.predict(
                        frame_cand_select, classes=[0], verbose=False, conf=0.15
                    )
                    current_detections_select = []  # list of (bbox_xyxy, temp_id)
                    if results_select and results_select[0].boxes:
                        temp_id_counter_select = 0
                        for box_select in results_select[0].boxes:
                            xyxy_select = box_select.xyxy[0].cpu().numpy()
                            current_detections_select.append(
                                (xyxy_select, temp_id_counter_select)
                            )
                            temp_id_counter_select += 1

                    display_frame_with_detections_colab(
                        frame_cand_select,
                        current_detections_select,
                        f"Candidate Frame (Reviewing): {frame_idx_select}",  # Changed label for clarity
                    )
                    frames_data_for_input_round.append(
                        (
                            frame_cand_select.copy(),
                            current_detections_select,
                            frame_idx_select,
                        )
                    )

                # --- Step 2: Collect inputs for all displayed frames in this round ---
                print(
                    "\\n\\n**********************************************************************"
                )
                print(
                    "**** ALL FRAMES FOR THIS ROUND DISPLAYED. PLEASE SCROLL UP TO REVIEW. ****"
                )
                print(
                    "**** NOW, PROVIDE INPUTS FOR EACH FRAME NUMBER REQUESTED BELOW. ****"
                )
                print(
                    "**********************************************************************\\n"
                )

                for (
                    frame_img_copy,
                    detections_list,
                    frame_idx_input,
                ) in frames_data_for_input_round:
                    # Check if quotas are already met before asking for this frame's input
                    if (
                        len(player1_confirmed_patches) >= MIN_CONFIRMATIONS
                        and len(player2_confirmed_patches) >= MIN_CONFIRMATIONS
                    ):
                        print(
                            "Quotas for P1 and P2 met. Skipping further input in this round."
                        )
                        break

                    print(
                        f"--- Input for Frame: {frame_idx_input} (Displayed Above) ---"
                    )

                    # Player 1 input for this frame
                    if len(player1_confirmed_patches) < MIN_CONFIRMATIONS:
                        while (
                            True
                        ):  # Loop for P1 input for this specific frame_idx_input
                            try:
                                # No plt.show() here, input should be stable
                                p1_input_str = (
                                    input(
                                        f"Enter temp ID for Player 1 in Frame {frame_idx_input} (or 'skip'): "
                                    )
                                    .strip()
                                    .lower()
                                )
                                if p1_input_str == "skip":
                                    print(
                                        f"  Player 1 skipped for Frame {frame_idx_input}."
                                    )
                                    break  # Break from P1 input for this frame_idx_input
                                p1_temp_id_select = int(p1_input_str)
                                p1_selected_bbox_this_frame = None
                                for (
                                    bbox_s,
                                    temp_id_s,
                                ) in (
                                    detections_list
                                ):  # Use detections_list passed for this frame
                                    if temp_id_s == p1_temp_id_select:
                                        p1_selected_bbox_this_frame = bbox_s
                                        break
                                if p1_selected_bbox_this_frame is not None:
                                    player1_confirmed_patches.append(
                                        (frame_img_copy, p1_selected_bbox_this_frame)
                                    )
                                    print(
                                        f"  Player 1 in Frame {frame_idx_input} confirmed with ID {p1_temp_id_select}. Total P1 confirmations: {len(player1_confirmed_patches)}."
                                    )
                                    break  # Break from P1 input for this frame_idx_input
                                else:
                                    print(
                                        f"  Invalid temporary ID {p1_temp_id_select} for Player 1 in Frame {frame_idx_input}. Try again."
                                    )
                            except ValueError:
                                print(
                                    "  Invalid input. Please enter a number or 'skip'."
                                )

                    # Player 2 input for this frame
                    if (
                        len(player2_confirmed_patches) < MIN_CONFIRMATIONS
                    ):  # Check P2 quota separately
                        while (
                            True
                        ):  # Loop for P2 input for this specific frame_idx_input
                            try:
                                # No plt.show() here
                                p2_input_str = (
                                    input(
                                        f"Enter temp ID for Player 2 in Frame {frame_idx_input} (or 'skip'): "
                                    )
                                    .strip()
                                    .lower()
                                )
                                if p2_input_str == "skip":
                                    print(
                                        f"  Player 2 skipped for Frame {frame_idx_input}."
                                    )
                                    break  # Break from P2 input for this frame_idx_input
                                p2_temp_id_select = int(p2_input_str)
                                p2_selected_bbox_this_frame = None
                                for (
                                    bbox_s,
                                    temp_id_s,
                                ) in detections_list:  # Use detections_list
                                    if temp_id_s == p2_temp_id_select:
                                        p2_selected_bbox_this_frame = bbox_s
                                        break
                                if p2_selected_bbox_this_frame is not None:
                                    player2_confirmed_patches.append(
                                        (frame_img_copy, p2_selected_bbox_this_frame)
                                    )
                                    print(
                                        f"  Player 2 in Frame {frame_idx_input} confirmed with ID {p2_temp_id_select}. Total P2 confirmations: {len(player2_confirmed_patches)}."
                                    )
                                    break  # Break from P2 input for this frame_idx_input
                                else:
                                    print(
                                        f"  Invalid temporary ID {p2_temp_id_select} for Player 2 in Frame {frame_idx_input}. Try again."
                                    )
                            except ValueError:
                                print(
                                    "  Invalid input. Please enter a number or 'skip'."
                                )

                cap_select.release()

                # Check if user wants to abort the whole process after a round
                if not (
                    len(player1_confirmed_patches) < MIN_CONFIRMATIONS
                    or len(player2_confirmed_patches) < MIN_CONFIRMATIONS
                ):
                    print("\nAll confirmations gathered!")
                    break  # Exit while loop for rounds
                else:
                    continue_selection = (
                        input(
                            "Continue to next round of player selection? (yes/no to skip entirely): "
                        )
                        .strip()
                        .lower()
                    )
                    if continue_selection != "yes":
                        print("User aborted player designation process.")
                        user_skipped_designation_entirely = True
                        break  # Exit while loop for rounds

            if not user_skipped_designation_entirely and (
                len(player1_confirmed_patches) >= MIN_CONFIRMATIONS
                and len(player2_confirmed_patches) >= MIN_CONFIRMATIONS
            ):
                player_tracker_instance.prime_with_multiple_user_selections(
                    player1_confirmed_patches, player2_confirmed_patches
                )
                print("Player tracker primed with multiple user selections.")
                user_did_select_players = (
                    True  # Flag that user selection was successful and completed
                )
            else:
                print(
                    "User-assisted player designation was skipped or did not meet minimum confirmations. Falling back to automatic assignment or Re-ID recovery if possible."
                )
                user_did_select_players = False  # Explicitly set to false

        elif enable_user_selection_main == "no":
            print(
                "User-assisted player selection disabled by choice. Using automatic assignment."
            )
            user_skipped_designation_entirely = True  # Equivalent to skipping
            user_did_select_players = False
        else:
            print(
                "Invalid input for enabling player selection. Defaulting to automatic assignment."
            )
            user_skipped_designation_entirely = True
            user_did_select_players = False

    except Exception as e_user_select_main:
        print(
            f"Error during multi-frame user-assisted player selection: {e_user_select_main}. Falling back to automatic player assignment."
        )
        user_skipped_designation_entirely = True
        user_did_select_players = False
    # --- End of New User-assisted Player Designation System ---

    # --- Pass 1: Ball tracking ---
    print("Starting Pass 1: Ball tracking...")
    time_pass1_start = time.time()  # Start of Pass 1 timing
    ball_track_all = []  # Initialize ball_track_all for Pass 1

    # Re-initialize main video capture for Pass 1
    cap = cv2.VideoCapture(final_input)
    if not cap.isOpened():
        print(f"Error: Could not open video {final_input} for Pass 1")
        sys.exit()

    # Use globally determined fps and total_frames, but re-fetch for this specific cap instance just in case
    fps = cap.get(cv2.CAP_PROP_FPS)
    if not fps or fps <= 0:
        print(
            f"Warning: Invalid FPS: {fps} from main cap. Using {fps_for_selection} FPS."
        )
        fps = fps_for_selection  # Use the one determined earlier

    # GET frame_width and frame_height HERE
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames == 0 and total_frames_for_selection > 0:
        print(
            f"Warning: Main cap reported 0 total frames. Using {total_frames_for_selection} from pre-check."
        )
        total_frames = total_frames_for_selection
    elif total_frames == 0:
        print(
            "CRITICAL ERROR: total_frames is 0 from all checks. Cannot proceed with progress bars or duration calculation."
        )
        # Allow to proceed but some logging might be off

    # Add print statement for clarity, similar to original
    print(
        f"Input video for processing: {frame_width}x{frame_height} @ {float(fps):.2f} FPS, Total Frames: {total_frames}"
    )

    temp_frame_count_pass1 = 0
    try:
        pbar_pass1 = tqdm(
            total=total_frames,
            desc="Pass 1: Ball Tracking",
            file=sys.stdout,
            dynamic_ncols=True,
            leave=False,
        )
    except NameError:
        pbar_pass1 = None
        print("tqdm not found, proceeding without progress bar for Pass 1.")

    while True:
        ret, frame_pass1 = cap.read()  # Use distinct frame variable for clarity
        if not ret:
            break
        current_ball_pos_pass1 = ball_detector.infer_single(frame_pass1)
        ball_track_all.append(current_ball_pos_pass1)
        if pbar_pass1:
            pbar_pass1.update(1)
        temp_frame_count_pass1 += 1

    if pbar_pass1:
        pbar_pass1.close()
    time_pass1_end = time.time()  # End of Pass 1
    print(
        f"Pass 1: Ball tracking complete. Processed {len(ball_track_all)} frames for ball_track_all."
    )

    # Calculate bounces_all immediately after Pass 1, using ball_track_all
    bounces_all = set()
    time_bounce_detection_start = time.time()
    if DETECT_BOUNCES and bounce_detector.model is not None:
        print("Running bounce detection (after Pass 1)...")
        x_ball_for_bounce = [bp[0] if bp is not None else None for bp in ball_track_all]
        y_ball_for_bounce = [bp[1] if bp is not None else None for bp in ball_track_all]
        try:
            bounces_all = bounce_detector.predict(
                x_ball_for_bounce, y_ball_for_bounce, smooth=True
            )
        except Exception as e:
            print(f"Error during bounce detection: {e}")
            # bounces_all is already an empty set if an error occurs
    time_bounce_detection_end = time.time()

    cap.release()  # Release video capture after Pass 1
    # --- End of Pass 1 ---

    # --- Pass 2: Main processing, drawing, and writing (Original Main Loop) ---
    print("Starting Pass 2: Main processing and drawing...")
    cap = cv2.VideoCapture(final_input)  # Re-open video for second pass
    if not cap.isOpened():
        print(f"Error: Could not re-open video for Pass 2: {final_input}")
        sys.exit()

    out_main = None
    out_minimap = None
    bounce_color_og = (0, 255, 255)  # OG bounce color

    if ENABLE_DRAWING:
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out_main = cv2.VideoWriter(
            path_output_video, fourcc, float(fps), (frame_width, frame_height)
        )
        out_minimap = (
            cv2.VideoWriter(  # This will now contain accumulated bounces directly
                path_minimap_video, fourcc, float(fps), (MINIMAP_WIDTH, MINIMAP_HEIGHT)
            )
        )
        print(f"Output video enabled: {path_output_video}")
        print(f"Minimap video (with accumulated bounces) will be: {path_minimap_video}")
    else:
        print("Drawing disabled. Running in analytics-only mode.")

    # These lists will be populated during Pass 2
    homography_matrices_all = []
    kps_court_all = []
    persons_top_all = []
    persons_bottom_all = []
    # ball_track_all is already populated from Pass 1

    last_processed_homography = None
    last_processed_kps = None
    last_processed_persons_top = []  # This will store (bbox, center, display_name)
    last_processed_persons_bottom = []  # This will store (bbox, center, display_name)

    frame_count = -1  # Reset frame_count for Pass 2
    time_pass2_start = time.time()  # Start of Pass 2
    try:
        pbar_main_loop = tqdm(
            total=total_frames,
            desc="Pass 2: Processing Frames",
            file=sys.stdout,
            dynamic_ncols=True,
            leave=False,
        )
    except NameError:
        pbar_main_loop = None
        print("tqdm not found, proceeding without progress bar for Pass 2.")

    prev_frame_for_cut = None

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if pbar_main_loop:
            pbar_main_loop.update(1)

        current_homography = None
        current_kps = None
        # current_persons_top = [] # Not needed, derived from tracker
        # current_persons_bottom = [] # Not needed, derived from tracker

        current_ball_pos_from_pass1 = (
            ball_track_all[frame_count]
            if frame_count < len(ball_track_all)
            else (None, None)
        )

        tracked_players_current_frame = []  # List of (bbox, center, track_id)

        process_this_frame = frame_count % FRAME_PROCESSING_INTERVAL == 0

        if process_this_frame:
            current_homography, current_kps = court_detector.infer_single(
                frame, frame_count
            )
            if current_homography is not None:
                tracked_players_current_frame = player_tracker_instance.update(
                    frame, frame_num=frame_count, current_homography=current_homography
                )

                if (
                    not user_did_select_players and frame_count == 0
                ):  # Only call auto-assign if user didn't select AND it's the first frame eligible
                    print(
                        f"[Frame {frame_count}] Attempting initial automatic role assignment as user did not select or it's frame 0."
                    )
                    player_tracker_instance.assign_initial_player_roles(
                        tracked_players_current_frame, frame
                    )
                elif (
                    user_did_select_players
                    and not player_tracker_instance.role_to_track_id.get("Player 1")
                    and not player_tracker_instance.role_to_track_id.get("Player 2")
                ):
                    # If user selected, but roles are not yet matched to track IDs, try to assign.
                    # This allows assign_initial_player_roles to match primed features to live tracks.
                    # This should ideally happen over a few frames if not immediate.
                    # The `assign_initial_player_roles` is now modified to handle this.
                    print(
                        f"[Frame {frame_count}] User selected players. Attempting to match primed features to live tracks via assign_initial_player_roles."
                    )
                    player_tracker_instance.assign_initial_player_roles(
                        tracked_players_current_frame, frame
                    )

            last_processed_homography = current_homography
            last_processed_kps = current_kps
        else:
            current_homography = last_processed_homography
            current_kps = last_processed_kps
            tracked_players_current_frame = player_tracker_instance.update(
                frame, frame_num=frame_count, current_homography=current_homography
            )

        cut_detected = is_scene_cut(prev_frame_for_cut, frame, frame_idx=frame_count)

        prev_frame_for_cut = frame.copy()
        if cut_detected:
            player_tracker_instance.reassign_roles_after_cut(
                tracked_players_current_frame, frame
            )

        final_persons_top_this_frame = []
        final_persons_bottom_this_frame = []

        court_ref_instance_for_midline = CourtReference()
        net_y_on_ref_court = court_ref_instance_for_midline.net[0][1]

        if current_homography is not None and tracked_players_current_frame:
            for bbox, center_on_frame, track_id in tracked_players_current_frame:
                display_name = player_tracker_instance.get_player_display_name(track_id)

                is_top_half_player = False
                pt_on_frame_to_transform = np.array(
                    [[[float(center_on_frame[0]), float(center_on_frame[1])]]],
                    dtype=np.float32,
                )
                try:
                    mapped_center_on_ref_court = cv2.perspectiveTransform(
                        pt_on_frame_to_transform,
                        current_homography,
                    )
                    if mapped_center_on_ref_court is not None:
                        y_coord_on_ref = mapped_center_on_ref_court[0, 0, 1]
                        if y_coord_on_ref < net_y_on_ref_court:
                            is_top_half_player = True
                except cv2.error:
                    pass

                player_data_tuple = (
                    bbox,
                    center_on_frame,
                    display_name,
                )
                if is_top_half_player:
                    final_persons_top_this_frame.append(player_data_tuple)
                else:
                    final_persons_bottom_this_frame.append(player_data_tuple)

        homography_matrices_all.append(current_homography)
        kps_court_all.append(current_kps)
        persons_top_all.append(final_persons_top_this_frame)
        persons_bottom_all.append(final_persons_bottom_this_frame)

        if process_this_frame:
            last_processed_persons_top = final_persons_top_this_frame
            last_processed_persons_bottom = final_persons_bottom_this_frame
        else:
            pass

        if ENABLE_DRAWING and out_main is not None and out_minimap is not None:
            output_frame = frame.copy()
            minimap_frame_current = get_court_img()  # Fresh large minimap background

            # Homography for drawing on minimap for current frame_count:
            # This is the homography that was decided for this frame (either new or carried over).
            # It's the inverse homography (frame coordinates -> reference court coordinates)
            draw_homography_for_minimap = homography_matrices_all[frame_count]

            draw_kps_on_main_frame = current_kps  # KPS are already on frame coordinates

            # For drawing, use the latest identified players for this frame
            # These are from persons_top_all and persons_bottom_all for the current frame_count
            draw_persons_top_on_main_frame = persons_top_all[frame_count]
            draw_persons_bottom_on_main_frame = persons_bottom_all[frame_count]

            # Draw court lines on main video frame
            if draw_kps_on_main_frame is not None:
                draw_court_keypoints_and_lines(
                    output_frame, draw_kps_on_main_frame, frame_width, frame_height
                )

            # Draw live ball trace on MAIN output_frame
            if DRAW_TRACE:
                start_trace_main = max(0, frame_count - TRACE_LENGTH + 1)
                for j_main, idx_main in enumerate(
                    range(start_trace_main, frame_count + 1)
                ):
                    if (
                        idx_main < len(ball_track_all)
                        and ball_track_all[idx_main] is not None
                        and ball_track_all[idx_main][0] is not None
                    ):
                        px_main, py_main = ball_track_all[idx_main]
                        alpha_main = 1.0 - ((frame_count - idx_main) / TRACE_LENGTH)
                        color_fade_main = tuple(int(c * alpha_main) for c in light_blue)
                        cv2.circle(
                            output_frame,
                            (int(px_main), int(py_main)),
                            3,
                            color_fade_main,
                            -1,
                        )
            elif (
                current_ball_pos_from_pass1[0] is not None
            ):  # Draw current ball if not tracing
                bx_main, by_main = current_ball_pos_from_pass1
                cv2.circle(
                    output_frame, (int(bx_main), int(by_main)), 5, light_blue, -1
                )

            # Draw players on MAIN output_frame
            for (
                bbox,
                center_pt,
                display_name,
            ) in draw_persons_top_on_main_frame:  # Expects (bbox, center, display_name)
                if bbox is not None and len(bbox) == 4:
                    x1, y1, x2, y2 = map(int, bbox)
                    cv2.rectangle(output_frame, (x1, y1), (x2, y2), box_color, 2)
                    cv2.putText(
                        output_frame,
                        display_name,  # Use identified display_name
                        (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.7,
                        box_color,
                        2,
                    )
            for (
                bbox,
                center_pt,
                display_name,
            ) in (
                draw_persons_bottom_on_main_frame
            ):  # Expects (bbox, center, display_name)
                if bbox is not None and len(bbox) == 4:
                    x1, y1, x2, y2 = map(int, bbox)
                    cv2.rectangle(output_frame, (x1, y1), (x2, y2), box_color, 2)
                    cv2.putText(
                        output_frame,
                        display_name,  # Use identified display_name
                        (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.7,
                        box_color,
                        2,
                    )

            # --- Draw on MINIMAP (minimap_frame_current is the large reference court image) ---
            if (
                draw_homography_for_minimap is not None
            ):  # Ensure we have a homography for projections
                # 1. Ball trace on minimap (using ball_track_all from Pass 1)
                # The trace shows recent ball positions transformed onto the minimap.
                start_trace_minimap = max(0, frame_count - TRACE_LENGTH + 1)
                for j_minimap, idx_minimap in enumerate(
                    range(start_trace_minimap, frame_count + 1)
                ):
                    if (
                        idx_minimap < len(ball_track_all)
                        and ball_track_all[idx_minimap] is not None
                        and ball_track_all[idx_minimap][0]
                        is not None  # Check for valid coordinates
                    ):
                        px_ball_frame, py_ball_frame = ball_track_all[
                            idx_minimap
                        ]  # Ball coords in main video frame space
                        pt_ball_to_transform = np.array(
                            [[[float(px_ball_frame), float(py_ball_frame)]]],
                            dtype=np.float32,
                        )
                        try:
                            mapped_ball_trace_on_ref = cv2.perspectiveTransform(
                                pt_ball_to_transform, draw_homography_for_minimap
                            )
                            if mapped_ball_trace_on_ref is not None:
                                mx_ball_trace_ref = int(
                                    mapped_ball_trace_on_ref[0, 0, 0]
                                )
                                my_ball_trace_ref = int(
                                    mapped_ball_trace_on_ref[0, 0, 1]
                                )
                                alpha_minimap_trace = 1.0 - (
                                    (frame_count - idx_minimap) / TRACE_LENGTH
                                )
                                color_fade_minimap_trace = tuple(
                                    int(c * alpha_minimap_trace) for c in light_blue
                                )
                                # Draw ball trace on the large minimap_frame_current (reference court scale)
                                cv2.circle(
                                    minimap_frame_current,
                                    (mx_ball_trace_ref, my_ball_trace_ref),
                                    3,  # radius for trace
                                    color_fade_minimap_trace,
                                    20,  # thickness for trace (as requested)
                                )
                        except cv2.error:
                            pass  # Ignore perspective transform errors for ball trace elements

                # 2. Accumulated bounces on minimap (OG style)
                # bounces_all was calculated after Pass 1
                if DETECT_BOUNCES and bounces_all:
                    for bounce_frame_num in bounces_all:
                        if (
                            bounce_frame_num <= frame_count
                        ):  # If bounce happened on or before current frame_count
                            if bounce_frame_num < len(
                                ball_track_all
                            ):  # Ensure bounce_frame_num is a valid index
                                ball_pos_at_bounce_on_frame = ball_track_all[
                                    bounce_frame_num
                                ]
                                if (
                                    ball_pos_at_bounce_on_frame
                                    and ball_pos_at_bounce_on_frame[0] is not None
                                    and ball_pos_at_bounce_on_frame[1] is not None
                                ):
                                    bpx_frame, bpy_frame = (
                                        ball_pos_at_bounce_on_frame  # Bounce coords in main video frame space
                                    )
                                    pt_bounce_to_transform = np.array(
                                        [[[float(bpx_frame), float(bpy_frame)]]],
                                        dtype=np.float32,
                                    )

                                    # Use draw_homography_for_minimap (which is homography_matrices_all[frame_count])
                                    # to project past bounces onto the current frame's court perspective.
                                    try:
                                        mapped_bounce_on_ref_court = (
                                            cv2.perspectiveTransform(
                                                pt_bounce_to_transform,
                                                draw_homography_for_minimap,
                                            )
                                        )
                                        if mapped_bounce_on_ref_court is not None:
                                            mx_bounce_ref = int(
                                                mapped_bounce_on_ref_court[0, 0, 0]
                                            )
                                            my_bounce_ref = int(
                                                mapped_bounce_on_ref_court[0, 0, 1]
                                            )

                                            # Coordinates are on the large reference court scale (minimap_frame_current)
                                            if (
                                                0
                                                <= mx_bounce_ref
                                                < minimap_frame_current.shape[1]
                                                and 0
                                                <= my_bounce_ref
                                                < minimap_frame_current.shape[0]
                                            ):
                                                cv2.circle(
                                                    minimap_frame_current,
                                                    (mx_bounce_ref, my_bounce_ref),
                                                    10,  # OG radius
                                                    bounce_color_og,  # OG color (0,255,255)
                                                    40,
                                                )  # OG thickness
                                    except cv2.error:
                                        pass  # Ignore perspective transform errors for individual bounces

                # 3. Players on minimap
                # persons_top_all and persons_bottom_all contain data for current frame_count
                # These are persons detected on the main frame. Their center points need to be transformed.
                current_frame_persons_top_with_ids = persons_top_all[
                    frame_count
                ]  # Already has (bbox, center, display_name)
                current_frame_persons_bottom_with_ids = persons_bottom_all[frame_count]

                for (
                    bbox,
                    center_pt_player_frame,
                    display_name,
                ) in current_frame_persons_top_with_ids:
                    pt_player_to_transform = np.array(
                        [
                            [
                                [
                                    float(center_pt_player_frame[0]),
                                    float(center_pt_player_frame[1]),
                                ]
                            ]
                        ],
                        dtype=np.float32,
                    )
                    try:
                        mapped_player_on_ref_court = cv2.perspectiveTransform(
                            pt_player_to_transform, draw_homography_for_minimap
                        )
                        if mapped_player_on_ref_court is not None:
                            mx_player_ref, my_player_ref = int(
                                mapped_player_on_ref_court[0, 0, 0]
                            ), int(mapped_player_on_ref_court[0, 0, 1])
                            if (
                                0 <= mx_player_ref < minimap_frame_current.shape[1]
                                and 0 <= my_player_ref < minimap_frame_current.shape[0]
                            ):
                                cv2.circle(
                                    minimap_frame_current,
                                    (mx_player_ref, my_player_ref),
                                    player_minimap_radius,
                                    player_minimap_color,
                                    -1,
                                )
                    except cv2.error:
                        pass
                for (
                    bbox,
                    center_pt_player_frame,
                    display_name,
                ) in current_frame_persons_bottom_with_ids:
                    pt_player_to_transform = np.array(
                        [
                            [
                                [
                                    float(center_pt_player_frame[0]),
                                    float(center_pt_player_frame[1]),
                                ]
                            ]
                        ],
                        dtype=np.float32,
                    )
                    try:
                        mapped_player_on_ref_court = cv2.perspectiveTransform(
                            pt_player_to_transform, draw_homography_for_minimap
                        )
                        if mapped_player_on_ref_court is not None:
                            mx_player_ref, my_player_ref = int(
                                mapped_player_on_ref_court[0, 0, 0]
                            ), int(mapped_player_on_ref_court[0, 0, 1])
                            if (
                                0 <= mx_player_ref < minimap_frame_current.shape[1]
                                and 0 <= my_player_ref < minimap_frame_current.shape[0]
                            ):
                                cv2.circle(
                                    minimap_frame_current,
                                    (mx_player_ref, my_player_ref),
                                    player_minimap_radius,
                                    player_minimap_color,
                                    -1,
                                )
                    except cv2.error:
                        pass

            # Resize minimap_frame_current (which now has trace, accumulated bounces, players) and write it
            minimap_resized = cv2.resize(
                minimap_frame_current, (MINIMAP_WIDTH, MINIMAP_HEIGHT)
            )
            out_minimap.write(minimap_resized)

            # Overlay minimap on main output frame
            output_frame[0:MINIMAP_HEIGHT, 0:MINIMAP_WIDTH] = minimap_resized
            out_main.write(output_frame)

    if pbar_main_loop:
        pbar_main_loop.close()
    time_pass2_end = time.time()  # End of Pass 2
    print("Finished frame processing.")  # This refers to Pass 2

    cap.release()  # Release after Pass 2

    if out_main:
        out_main.release()
    if out_minimap:
        out_minimap.release()

    # bounces_all is already computed from Pass 1.
    # The section for calling add_bounces_to_minimap_video is already commented out.

    time_heatmap_start = time.time()
    if GENERATE_HEATMAPS:
        print("Generating heatmaps...")
        if not homography_matrices_all or not ball_track_all:
            print(
                "Warning: Cannot generate heatmaps due to missing homography or ball track data."
            )
        else:
            try:
                generate_minimap_heatmaps(
                    homography_matrices=homography_matrices_all,
                    ball_track=ball_track_all,
                    bounces=bounces_all,
                    persons_top=persons_top_all,
                    persons_bottom=persons_bottom_all,
                    output_bounce_heatmap=path_output_bounce_heatmap,
                    output_player_heatmap=path_output_player_heatmap,
                    blur_ksize=41,
                    alpha=0.5,
                )
            except Exception as e:
                print(f"Error generating heatmaps: {e}")
    time_heatmap_end = time.time()

    total_script_time = time.time() - start_time
    print(f"--- Processing Complete ---")

    # Performance Metrics Calculation
    video_duration_seconds = total_frames / fps if fps > 0 else 0
    time_taken_scaling = time_scaling_end - time_scaling_start
    time_taken_pass1 = time_pass1_end - time_pass1_start  # Need time_pass1_start
    time_taken_bounce_detection = (
        time_bounce_detection_end - time_bounce_detection_start
    )
    time_taken_pass2 = time_pass2_end - time_pass2_start
    time_taken_heatmaps = time_heatmap_end - time_heatmap_start

    print("\n--- Performance Metrics ---")
    print(
        f"Input Video Duration: {video_duration_seconds:.2f} seconds ({total_frames} frames @ {fps:.2f} FPS)"
    )
    print(f"Time for Video Scaling (if any): {time_taken_scaling:.2f} seconds")
    print(f"Time for Pass 1 (Ball Tracking): {time_taken_pass1:.2f} seconds")
    if total_frames > 0 and time_taken_pass1 > 0:
        print(f"  Pass 1 Processing Speed: {total_frames / time_taken_pass1:.2f} FPS")
    if DETECT_BOUNCES:
        print(f"Time for Bounce Detection: {time_taken_bounce_detection:.2f} seconds")
    print(
        f"Time for Pass 2 (Main Processing & Drawing): {time_taken_pass2:.2f} seconds"
    )
    if total_frames > 0 and time_taken_pass2 > 0:
        print(f"  Pass 2 Processing Speed: {total_frames / time_taken_pass2:.2f} FPS")
    if GENERATE_HEATMAPS:
        print(f"Time for Heatmap Generation: {time_taken_heatmaps:.2f} seconds")

    print(f"Total Script Execution Time: {total_script_time:.2f} seconds")
    if video_duration_seconds > 0 and total_script_time > 0:
        # Effective FPS considering the whole script time relative to video duration
        # This includes file I/O, model loading, all passes, etc.
        overall_processing_fps = total_frames / total_script_time
        print(
            f"Overall Effective Processing Speed: {overall_processing_fps:.2f} FPS (video frames processed per second of wall clock time)"
        )
        print(
            f"Processing Time Ratio (Script Time / Video Duration): {total_script_time / video_duration_seconds:.2f}x"
        )

    if ENABLE_DRAWING:
        print(f"Output video saved to: {path_output_video}")
        # Path_minimap_video now refers to the one potentially with bounces
        print(f"Minimap video saved to: {path_minimap_video}")
    if DETECT_BOUNCES:
        print(f"Bounces detected: {len(bounces_all)}")
    if GENERATE_HEATMAPS:
        print(f"Bounce heatmap saved to: {path_output_bounce_heatmap}")
        print(f"Player heatmap saved to: {path_output_player_heatmap}")
    # print(f"Total execution time: {total_script_time:.2f} seconds") # Replaced by detailed metrics
