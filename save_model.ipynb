{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFRav6nzZERxacDn/V2dxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corypham/CourtCheck/blob/main/save_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfuyGD9ZLMsM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "\n",
        "# Unregister the dataset if it already exists\n",
        "def unregister_dataset(dataset_name):\n",
        "    if dataset_name in DatasetCatalog.list():\n",
        "        DatasetCatalog.pop(dataset_name)\n",
        "        MetadataCatalog.pop(dataset_name)\n",
        "\n",
        "# Define keypoint metadata\n",
        "keypoint_names = [\n",
        "    \"BTL\", \"BTLI\", \"BTRI\", \"BTR\", \"BBR\", \"BBRI\", \"IBR\", \"NR\", \"NM\", \"ITL\",\n",
        "    \"ITM\", \"ITR\", \"NL\", \"BBL\", \"IBL\", \"IBM\", \"BBLI\"\n",
        "]\n",
        "\n",
        "keypoint_flip_map = [\n",
        "    (\"BTL\", \"BTR\"), (\"BTLI\", \"BTRI\"), (\"BBL\", \"BBR\"), (\"BBLI\", \"BBRI\"), (\"ITL\", \"ITR\"),\n",
        "    (\"ITM\", \"ITM\"), (\"NL\", \"NR\"), (\"IBL\", \"IBR\"), (\"IBM\", \"IBM\"), (\"NM\", \"NM\")\n",
        "]\n",
        "\n",
        "skeleton = []\n",
        "\n",
        "# Custom trainer with evaluator\n",
        "class TrainerWithEval(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "\n",
        "# Function to set up and train the model with mixed datasets incrementally\n",
        "def train_model_incrementally(game_numbers, max_iter, resume=False):\n",
        "    for game_number in game_numbers:\n",
        "        unregister_dataset(f\"tennis_game{game_number}_train\")\n",
        "        unregister_dataset(f\"tennis_game{game_number}_val\")\n",
        "\n",
        "        json_train_file = f\"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/annotations/model_annotations/games/game{game_number}/game{game_number}_train.json\"\n",
        "        json_val_file = f\"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/annotations/model_annotations/games/game{game_number}/game{game_number}_val.json\"\n",
        "        image_root_train = f\"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/dataset/game{game_number}/game{game_number}_train\"\n",
        "        image_root_val = f\"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/dataset/game{game_number}/game{game_number}_val\"\n",
        "        register_coco_instances(f\"tennis_game{game_number}_train\", {}, json_train_file, image_root_train)\n",
        "        register_coco_instances(f\"tennis_game{game_number}_val\", {}, json_val_file, image_root_val)\n",
        "\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_train\").keypoint_names = keypoint_names\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_train\").keypoint_flip_map = keypoint_flip_map\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_train\").keypoint_connection_rules = skeleton\n",
        "\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_val\").keypoint_names = keypoint_names\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_val\").keypoint_flip_map = keypoint_flip_map\n",
        "        MetadataCatalog.get(f\"tennis_game{game_number}_val\").keypoint_connection_rules = skeleton\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(\"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.DATASETS.TRAIN = tuple([f\"tennis_game{game_number}_train\" for game_number in game_numbers])\n",
        "    cfg.DATASETS.TEST = tuple([f\"tennis_game{game_number}_val\" for game_number in game_numbers])\n",
        "    cfg.DATALOADER.NUM_WORKERS = 4\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 4  # increase if you have more GPU memory\n",
        "    cfg.SOLVER.BASE_LR = 0.0001  # lower learning rate for more careful training\n",
        "    cfg.SOLVER.MAX_ITER = max_iter  # total number of iterations\n",
        "    cfg.SOLVER.STEPS = [int(max_iter*0.75), int(max_iter*0.875)]  # decay learning rate\n",
        "    cfg.SOLVER.GAMMA = 0.1  # decay factor\n",
        "    cfg.SOLVER.CHECKPOINT_PERIOD = 500  # save a checkpoint every 500 iterations\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  # increase for more stable gradients\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 11  # your dataset has 11 classes\n",
        "\n",
        "    output_dir = \"/content/drive/MyDrive/ASA Tennis Bounds Project/models/court_detection_model/detectron2/game_model\"\n",
        "    cfg.OUTPUT_DIR = output_dir\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    trainer = TrainerWithEval(cfg)\n",
        "    trainer.resume_or_load(resume=resume)\n",
        "    trainer.train()\n",
        "\n",
        "# Train with mixed datasets incrementally\n",
        "# train_model_incrementally([1], 8000, resume=False)        # Train with game 1 to 8000 iterations (initial training)\n",
        "# train_model_incrementally([1, 2], 16000, resume=True)     # Continue training with games 1 and 2 to 16000 iterations\n",
        "# train_model_incrementally([1, 2, 3], 24000, resume=True)  # Continue training with games 1, 2, and 3 to 24000 iterations\n",
        "train_model_incrementally([1, 2, 3, 4], 44000, resume=True)\n",
        "# train_model_incrementally([1, 2, 3, 4, 5], 40000, resume=True)\n",
        "# train_model_incrementally([1, 2, 3, 4, 5, 6], 48000, resume=True)\n",
        "# train_model_incrementally([1, 2, 3, 4, 5, 6, 7], 56000, resume=True)\n",
        "# train_model_incrementally([1, 2, 3, 4, 5, 6, 7, 8], 64000, resume=True)\n",
        "# train_model_incrementally([1, 2, 3, 4, 5, 6, 7, 8, 9], 72000, resume=True)\n"
      ]
    }
  ]
}